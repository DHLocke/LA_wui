---
title: "Woosley land cover"
author: "Dexter H. Locke, PhD"
date: "`r format(Sys.time())`"
output: html_document
editor_options: 
  chunk_output_type: console
---

DUPE BUILDINGS STILL ?!?!?

TODO We want to drop parcels without any building on them for the building analyses.


For JOD:
  WHATS UP WITH BARE SOIL? 
    Is this a classifcation problem?

year 2008 is important for policy reasons.
  Consider a before and after variable. 


<!-- Effective Year 1 -->
<!--   zero and NA should become NA. (random forest can handel this, regression maybe not.) -->
<!--   need to investigate -->
<!--   make a most recent year built var? -->

  Roll_ImpValue - might indicate
  Roll_HomeOwnersExempt - owner occupied?

TODO: 
Variable selection  
1. Per buffered landscape characteristic, which one has the greatest assocition with the outcome (binary)
  *Deviance* explained will be called the most important. 
    Would be a great supplement for a paper on its own.

Donish  
2. Correlation matrix for continuous predictors
      cut off around 0.8 - non-observed
      
3. If still unweildy, consider grouping vars
  ex flammable veg
      
Mirand:
  check out building buffers (is it centroid or perimeter.)


2) Build dist building has an outlier that makes it hard to see box plot

cool. But, why isn't year built significant? what is happening?? also, aren't the two road variables correlated?

3) why does build_has_tree_overhang box plot look so weird? Sum stats say mean is 0.7 but these values are either 0 or 1?



Done
Buildings only from the damaged dataset, are they redundant? Drop the blanks?
  - TODO Need to cut redundant fileds

Done
Parcels Join buildings
  - st_join is fine.
  - DO IT NOW
  
Done
dependent variable
  - apply the reclassification (the case_when business)

Done
show model preliminaries~~

Done
Get discriptive statistics
  cross-tabulations with the binary outcome - which ones? (boxplots?)
  sort by variable (like group all soils together.)
  
look at importance values for an all-variables random forest
  (do not use variable selection)

Done
logisitc regression with handfull of puposively selected variables
  A veg around house
  position on landscae
    building density
    aspect
    roads
  B what buffer distance seems most related to   

What's up with year built? (Miranda to investigate) - in progress
  Use effective 


Next steps
  - Census data
    * multi-level logistic regression
  - analyses of just those affected/ damaged/ destroyed/ major/ minor


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



## 0 set up: load libraries, custom functions, set defaults
```{r}
# load libraries
# packages we'll be using
packs <- c('tidyverse',      # a must have!
           'tidylog',        # makes things very verbose for 2x checking 
           'magrittr',       # all of the pipes
           'janitor',        # cleans things up
           'sf',             # simple features
           'mapview',        # quick webmaps for zoom/pan viz
           'tidycensus',     # access to Census data in a tidy way) 
           'party',          # random forests
           'psych',          # describe is very useful for descriptive statistics
           'sjPlot')         # useful plotting and regression support

# check for all of the libraries
if (length(setdiff(packs, rownames(installed.packages()))) > 0) {
  install.packages(setdiff(packs, rownames(installed.packages())))  
}

lapply(packs, library, character.only = TRUE)


# changing from tidyr v0.8.3 -> v1.0.0 broke things for nest and unnest!
# https://tidyr.tidyverse.org/articles/in-packages.html#tidyr-v0-8-3---v1-0-0
library(tidyr)
nest <- nest_legacy                     # Why Hadley, WHY!
unnest <- unnest_legacy


# set custom function for getting spatial data
see_sf <- function(){
# what's in memory that are sf - spatial features?
keep(eapply(.GlobalEnv, class),      # gets the objects in the global environment
     ~ any(str_detect(., "sf"))) %>% # selects elements with sf in them
names(.) %>% as.character(.)       # my simple features
}

see_sf() -> sf_in_memory

## what are the spatial references of those SF classes?
mget(sf_in_memory) %>%
purrr::map(~st_crs(.x)$epsg) %>% unlist() #%>% View()

# NOT IN
`%nin%` <- Negate(`%in%`) # custom function

# where are we?
list.files()
list.files('data')

# parameter to keep in general, for later..
pxl_to_ft_conversion <- 0.75
# pxl_to_ft_conversion <- 0.2286004572

set.seed(19870630)

## Make a 'figures' subdirectory if one doesn't exist
ifelse(!dir.exists(file.path('figures')), dir.create(file.path('figures')), FALSE)
```


## 1 bring in new GIS data from Miranda, this will be the base from now on (August, 2020)
```{r}
# GIS DATA from MIRANDA
st_layers(paste0(getwd(), '/data/Outgoing_BuildingSumms_DHL20200423.gdb'))

# this will be the main building layer!
# left-most columns are building-specific summaries
# then there are building attributes for select buildings (from Alex' DINS data)
# already done by M Mockrin - Thank you!
# next attributes will be joined from  Jarlath's building info
# after we can add parcel-level variables
# eventually we'll get block group-level data into the buildings, too.
build <- read_sf(dsn = paste0(getwd(), '/data/Outgoing_BuildingSumms_DHL20200423.gdb'),
                      layer = 'Buildings_ParcelID_DINS',
                      quiet = FALSE) %>% 
  tidylog::select(ID_Build : Mean_distall_road,
                  structures_DAMAGE = Buildings_ParcelID_DINS_intersects_DAMAGE, # renaming
                  starts_with('NEAR_'),
                  Shape_Length, Shape_Area) %>% 
  mutate(damage_cat =
           as.factor(
           case_when(is.na(structures_DAMAGE) ~ 'Survived',
                     structures_DAMAGE == 'Affected (1-9%)' ~ 'Survived',
                     structures_DAMAGE == 'Minor (10-25%)' ~ 'Survived',
                     structures_DAMAGE == 'Major (26-50%)' ~ 'Destroyed',
                     structures_DAMAGE == 'Destroyed (>50%)' ~ 'Destroyed')),
         damage_binary = ifelse(damage_cat == 'Destroyed', 1, 0),
         damage_severity = 
           case_when(
               is.na(structures_DAMAGE) ~ 1,
                     structures_DAMAGE == 'Affected (1-9%)' ~ 2,
                     structures_DAMAGE == 'Minor (10-25%)' ~ 3,
                     structures_DAMAGE == 'Major (26-50%)' ~ 4,
                     structures_DAMAGE == 'Destroyed (>50%)' ~ 5))

# define & refine the dependent variable versions: numeric (0/1), categorical (survived/destroyed),
# and severeity?
build %>% st_drop_geometry() %>% tabyl(structures_DAMAGE) #%>% 
#  write.csv(., paste0(getwd(), '/output_data/damage_cats.csv'), row.names = FALSE)

build %>% st_drop_geometry() %>% tabyl(structures_DAMAGE, damage_binary) #%>% 
  #write.csv(., paste0(getwd(), '/output_data/damage_cats_binary.csv'), row.names = FALSE)

build %>% st_drop_geometry() %>% tabyl(structures_DAMAGE, damage_binary) #%>%
  # adorn_percentages('col') %>% adorn_pct_formatting() %>% 
  # write.csv(., paste0(getwd(), '/output_data/damage_cats_binary_pct_form.csv'),
  #           row.names = FALSE)

build %>% st_drop_geometry() %>% tabyl(damage_binary)
build %>% st_drop_geometry() %>% tabyl(damage_severity)

build %>% st_drop_geometry() %>% tabyl(damage_severity, damage_binary) #%>% 
  #write.csv(., paste0(getwd(), '/output_data/damage_binary_severity.csv'), row.names = FALSE)

glimpse(build)
```



## 2 bring in building attributes --from Jarlath-- (start simple at first) 
### 2a building distance to building
```{r}
# BUILDING DISTANCE TO BUILDING
data_path <- 'data/WoosleyFire_eCogOuput_06Dec2019/Building_dist_Building'
data_path # building dist build

# update? https://twitter.com/kc_analytics/status/1345434428059881480/photo/2
files <- dir(data_path, recursive = TRUE, pattern = "*.csv"); files # get file names


build_dist_to_build <-
  tibble(filename = files) %>% # create a data frame, holding the file names
  mutate(file_contents = map(filename,          # read files into
                             ~ read.csv(file.path(data_path, .),
                                              #fileEncoding='latin1',
                                        colClasses = c('character',
                                                       'character',
                                                       'factor',
                                                       'factor',
                                                       'character',
                                                       'character'),
                                        sep = ';'))) %>%  # a new data column
  unnest() %>%
  clean_names() %>%
  mutate(dist_to_build_ft = pxl_to_ft_conversion*as.numeric(distance_to_buildings_pxl),
         dist_to_build_ft = ifelse(is.na(dist_to_build_ft), 0, dist_to_build_ft),
         ID_Build = as.numeric(id_build_buildings)) %>%
  select(ID_Build,
         dist_to_build_ft) %>%
  filter(!is.na(ID_Build)) %>% 
  distinct() %>% 
  group_by(ID_Build) %>% 
  arrange(dist_to_build_ft) %>%    # sorting by shortest distance to building
  slice(1) %>%                  # grabs the first record
  ungroup()

# No NA's woohooo
build_dist_to_build; summary(build_dist_to_build)

length(unique(build_dist_to_build$ID_Build)); dim(build_dist_to_build)
build_dist_to_build %>% 
  group_by(ID_Build) %>%
  tally() %>%
  filter(n > 1) %>% 
  arrange(desc(n)) -> dup_build_ids # success!
  # View()

sum(dup_build_ids$n) # no duplicates, nice


# build_dist_to_build %>% 
#   filter(ID_Build %in% dup_build_ids$ID_Build) %>% View()

# LEFT
build %<>% 
  left_join(build_dist_to_build, by = c('ID_Build' = 'ID_Build')) # 48 bad matches.. 
```



### 2b building tree overhang
```{r}
data_path <- 'data/WoosleyFire_eCogOuput_06Dec2019/Building_TreeOverhang'
data_path # building tree overhang

files <- dir(data_path, recursive = TRUE, pattern = "*.csv"); files # get file names

build_tree_overhang <-
  tibble(filename = files) %>% # create a data frame, holding the file names
  mutate(file_contents = map(filename,  # read files into
                             ~ read.csv(file.path(data_path, .),
                                        colClasses = c('character',
                                                       'character',
                                                       'character',
                                                       'character',
                                                       'character',
                                                       'numeric',
                                                       'character'),
                                        sep = ';'))) %>%  # a new data column
  unnest() %>%
  clean_names() %>% 
  mutate(has_tree_overhang = ifelse(mean_n_dsm_tree_canopy_sub_objects == 'undefined',
                                    0, 1),
         overhang_ht = as.numeric(ifelse(mean_n_dsm_tree_canopy_sub_objects != 'undefined',
                                         mean_n_dsm_tree_canopy_sub_objects, NA)),
         overhang_ht = ifelse(is.na(overhang_ht), 0, overhang_ht),
         perc_overhang = rel_area_of_sub_objects_tree_canopy_1) %>% 
  select(ID_Build = id_build_buildings,
         has_tree_overhang,
         overhang_ht,
         perc_overhang) %>% 
  filter(ID_Build != 'undefined') %>% 
  mutate(ID_Build = as.numeric(ID_Build)) %>% # could move this up, save some code.. like the chunck above.
  group_by(ID_Build) %>% 
  arrange(desc(perc_overhang)) %>%    # sorting to get the most overhang. See ID_Build == 6
  slice(1) %>%                        # grabs the first record
  ungroup()

# unnecesary if last chunk ends with "distinct"
build_tree_overhang %>% 
  group_by(ID_Build) %>% 
  tally() %>% 
  filter(n > 1) %>% 
  arrange(desc(n)) -> dup_build_tree_overhang # no dups because the largest % overhang chosen

# build_tree_overhang %>% 
#   filter(ID_Build %in% dup_build_tree_overhang$ID_Build) %>% View()

# semi-join because of duplicates here
# in building distance to building its ok to take the shortest, as was done above
build %<>% 
  left_join(build_tree_overhang, by = c('ID_Build' = 'ID_Build')) 
```


### 2c building land cover buffer
```{r}
data_path <- 'data/WoosleyFire_eCogOuput_06Dec2019/Building_LandCoverBuff'
data_path # building dist build

files <- dir(data_path, recursive = TRUE, pattern = "*.csv"); files # get file names

build_tree_landcover_buff <-
  tibble(filename = files) %>% # create a data frame, holding the file names
  mutate(file_contents = map(filename,  # read files into
                             ~ read.csv(file.path(data_path, .),
                                        colClasses = c('character'),
                                        sep = ';'))) %>%  # a new data column
  unnest() %>%
  clean_names() %>% 
  select(ID_Build = id_build_building_centroids, 
         ID_Parcel = id_parcel_parcels,
         treecan10 = rel_area_of_tree_canopy_13, #tree
         treecan100 = rel_area_of_tree_canopy_133,
         treecan200 = rel_area_of_tree_canopy_267,
         treecan300 = rel_area_of_tree_canopy_400,
         grass10 = rel_area_of_grass_13,       #grass
         grass_100= rel_area_of_grass_133,
         grass_200= rel_area_of_grass_267,
         grass_300= rel_area_of_grass_400,
         soil_10 = rel_area_of_bare_soil_13,   # soil
         soil_100= rel_area_of_bare_soil_133,
         soil_200= rel_area_of_bare_soil_267,
         soil_300= rel_area_of_bare_soil_400,
         water_10= rel_area_of_water_13,       # water
         water_100=rel_area_of_water_133, 
         water_200=rel_area_of_water_267,
         water_300=rel_area_of_water_400,
         build_10 = rel_area_of_buildings_13,   # building
         build_100= rel_area_of_buildings_133,
         build_200= rel_area_of_buildings_267,
         build_300= rel_area_of_buildings_400,
         road_10 = rel_area_of_roads_13,       # road
         road_100= rel_area_of_roads_133,
         road_200= rel_area_of_roads_267,
         road_300= rel_area_of_roads_400,
         oth_imp_10 = rel_area_of_other_impervious_13, #other impervious
         oth_imp_100= rel_area_of_other_impervious_133,
         oth_imp_200= rel_area_of_other_impervious_267,
         oth_imp_300= rel_area_of_other_impervious_400,
         shrub_10 = rel_area_of_shrub_13,       # shrub
         shrub_100= rel_area_of_shrub_133,
         shrub_200= rel_area_of_shrub_267,
         shrub_300= rel_area_of_shrub_400) %>% 
   # filter(ID_Build != 'undefined') %>% 
  mutate_if(is.character, as.numeric) %>% 
  mutate_at(vars(-ID_Build, -ID_Parcel), list(~. * 100)) %>% # filter(ID_Build != 'undefined')
  select(-ID_Parcel)

build %<>% 
  left_join(build_tree_landcover_buff,
            by = c('ID_Build' = 'ID_Build')) %>% 
  rename_at(vars(starts_with('Mean_')), ~(str_replace(., 'Mean_', 'build_Mean'))) %>% 
  #rename_at(vars(starts_with('damage_')), ~(str_replace(., 'damage_', 'structures_damage_'))) %>% 
  rename_at(vars(dist_to_build_ft : shrub_300), ~paste0('build_', .))


glimpse(build)
```


### 2d double check land cover values
```{r eval=FALSE, include=FALSE}
build %>% mapview(., zcol = 'build_build_10')

# build %>% st_drop_geometry() %>% 
#   write.csv(., paste0(getwd(), '/output_data/test_joiner_',
#                       gsub('[[:punct:]]', '_', Sys.time()), '.csv'),
#             row.names = TRUE)
```


## 3 parcels
```{r}


<!-- Effective Year 1 -->
<!--   zero and NA should become NA. (random forest can handel this, regression maybe not.) -->
<!--   need to investigate -->
<!--   make a most recent year built var? -->
<!--   Include Year Built IN ADDITION to Effective Year built -->
  
  Roll_ImpValue - might indicate
  Roll_HomeOwnersExempt - owner occupied?


# Miranda's parcels
parcel_complete_atts     <- read_sf(dsn = paste0(getwd(), '/data/Outgoing_BuildingSumms_DHL20200423.gdb'),
                  layer = 'Parcels_fireperim_LC',
                  quiet = FALSE)
  
parcel <- read_sf(dsn = paste0(getwd(), '/data/Outgoing_BuildingSumms_DHL20200423.gdb'),
                  layer = 'Parcels_fireperim_LC',
                  quiet = FALSE) %>% # glimpse
  select(AIN, UseType, UseDescription, Roll_ImpValue, Roll_HomeOwnersExemp,
         Can_P : Imperv_P, # ) %>%  #,
         starts_with('EffectiveYear'),
         #starts_with('YearBuilt')
         ) %>%         # FIXME ? Do we want area, too?
  mutate_at(vars(starts_with("EffectiveYear")), as.numeric) %>% 
  # mutate_at(vars(starts_with("EffectiveYear")), ~ifelse(. == 0, NA, .)) %>% 
  # mutate_at(vars(starts_with("YearBuilt")),     as.numeric) %>% 
  # mutate_at(vars(starts_with("YearBuilt")),     ~ifelse(. == 0, NA, .)) %>% 
  rename_at(vars(-Shape), ~paste0('parcel_',.)) # %>% glimpse()


parcel %>% 
  select(parcel_AIN,
         starts_with("parcel_EffectiveYear")) %>% 
         #starts_with("parcel_YearBuilt")) %>%
  st_drop_geometry() #%>% View()

# parcel %>% filter(parcel_EffectiveYear1 == parcel_YearBuilt1)
# parcel %>% filter(parcel_EffectiveYear1  > parcel_YearBuilt1)
# parcel %>% filter(parcel_EffectiveYear1  < parcel_YearBuilt1)

# create most-recent year build column
(
  parcel %>% 
  select(parcel_AIN,
         starts_with("parcel_EffectiveYear") #, starts_with("parcel_YearBuilt")
         ) %>%
  st_drop_geometry() %>% 
  pivot_longer(cols = -parcel_AIN, names_to = 'year') %>% #arrange(desc(value))
  group_by(parcel_AIN) %>% 
  summarise(parcel_recent = value[which.max(value)]) %>% 
  ungroup() -> parcel_recent
)

# join that back
parcel %<>% left_join(., parcel_recent)

# which ones didn't join completely?
parcel_na_recent <- parcel %>% filter(is.na(parcel_recent)) %>% 
  select(parcel_AIN:parcel_Roll_HomeOwnersExemp, parcel_Build_P) # these records don't have year built info


# where are they?
# parcel_na_recent %>% mapview()

# what do they look like
#parcel_na_recent %>% st_drop_geometry() %>% View()

# of those witihout year built info
# how many also have no buildings or tax info that might indicate building?
# make these zero for year built?
parcel_na_recent %>%
  filter(parcel_Roll_ImpValue == 0 &
           parcel_Roll_HomeOwnersExemp == 0 &
           parcel_Build_P == 0) -> parcel_na_recent_zeros  # year built should be zero for these
# TODO: MAKE these zeros.

# Where are they located?
parcel_na_recent %>%
  filter(parcel_AIN %nin% parcel_na_recent_zeros$parcel_AIN) %>% # SUPPOSED TO HAVE YEAR BUILT
  mapview()

# # using all of the attribute data for additional clues
# parcel_complete_atts %>% 
#   filter(AIN %in% parcel_na_recent_zeros$parcel_AIN) %>% 
#   mapview()

# what are the land uses of these that plaussibly ought to be zeros?
parcel_na_recent %>%
  filter(parcel_AIN %nin% parcel_na_recent_zeros$parcel_AIN) %>% # SUPPOSED TO HAVE YEAR BUILT
  tabyl(parcel_UseDescription) %>% 
  as_tibble() %>% 
  arrange(desc(n))


parcel_na_recent %>%
  filter(parcel_AIN %in% parcel_na_recent_zeros$parcel_AIN) %>% # ok to have not year built, in theory
  tabyl(parcel_UseDescription) %>% 
  as_tibble() %>% 
  arrange(desc(n))



parcel %>% 
  select(starts_with("parcel_Effective"),
         #starts_with("parcel_YearBuilt"),
         parcel_recent) %>% # are these NA parcels undeveloped
  map(., ~sum(is.na(.)))

# missing years do occure in the residential land uses.
parcel %>% st_drop_geometry() %>%  tabyl(parcel_UseType, parcel_recent)

parcel %>% st_drop_geometry() %>%  filter(parcel_Roll_ImpValue == 0)
parcel %>% st_drop_geometry() %>%  tabyl(parcel_Roll_ImpValue, parcel_recent)
parcel %>% filter(parcel_Roll_ImpValue == 0) %>% mapview()


# join buildings to their containing parcel
# takes about 1 minute!
system.time(
build %<>% st_join(parcel,           # spatial join
                   left = FALSE,     # if FALSE, only intersection be returned, instead of all
                   largest = TRUE) %>% 
  mutate(parcel_recent = ifelse(parcel_AIN %in% parcel_na_recent_zeros, 0, parcel_recent)))  # and attributes of intersection
                                     # largest = TRUE is AMAZING, but makes things much slower.
glimpse(build)
```


## 4 double checks
```{r}
# YEAR BUILT
# Only living quarters are given a year built info
build %>% 
  select(ID_Build, starts_with("parcel_Effective"), parcel_recent,
           #, starts_with("parcel_YearBuilt")
         ) %>% 
  st_drop_geometry() %>% 
  mutate_if(is.character, as.numeric) %>% #summary()
  pivot_longer(cols = -ID_Build, names_to = 'variable') %>% 
  ggplot(aes(value)) + 
  geom_density() + 
  facet_wrap(~variable, scales = 'free_x') + 
  theme_bw()



# ggsave(file = paste0(getwd(), '/figures/effective_year_',
#                     gsub('[[:punct:]]', '_', Sys.time()), '.png'))

build %>% 
  select(starts_with("parcel_Effective"),
         # starts_with("parcel_YearBuilt"),
         parcel_recent) %>% map(., ~sum(is.na(.)))

# how many of these 
build %>%
  filter(parcel_Roll_ImpValue == 0 &
           parcel_Roll_HomeOwnersExemp == 0 &
           parcel_Build_P == 0) %>% # year built should be zero for these
  glimpse()

# where are these NA's?
build %>% filter(is.na(parcel_recent)) %>% mapview()
build %>% filter(parcel_recent == 0) %>% mapview()

summary(build$parcel_recent)

build %>% #st_drop_geometry() %>%
  ggplot(aes(parcel_recent)) +
  geom_histogram()

# ggsave(file = paste0(getwd(), '/figures/effective_yearRecent_',
#                     gsub('[[:punct:]]', '_', Sys.time()), '.png'))

# no longer needed, NA's zeros recoded in chunk above
# build %>% st_drop_geometry() %>% 
#   mutate(parcel_EffectiveYear1 = na_if(parcel_EffectiveYear1, 0)) %>% 
#   ggplot(aes(parcel_EffectiveYear1)) + geom_histogram()
# 
# ggsave(file = paste0(getwd(), '/figures/effective_yearONE_NO_ZERO',
#                     gsub('[[:punct:]]', '_', Sys.time()), '.png'))


build %>% st_drop_geometry() %>% 
  describe #%>%
  # write.csv(., paste0(getwd(), '/output_data/describe_',
  #                     gsub('[[:punct:]]', '_', Sys.time(), '.csv')),
  #           row.names = TRUE)

glimpse(build)

box_plot_dat <- build %>% 
  st_drop_geometry() %>% 
  select(damage_cat,
         starts_with('build_'),
         starts_with('parcel_')) %>% data.frame %>% glimpse

# for(i in 2:ncol(box_plot_dat)){
#   print(names(box_plot_dat)[i])
#   png(file = paste0(getwd(), '/figures/boxplot_', names(box_plot_dat)[i], '_',
#                     gsub('[[:punct:]]', '_', Sys.time()), '.png'))
#   boxplot(box_plot_dat[,i] ~ box_plot_dat$damage_cat, main = names(box_plot_dat)[i])
#   dev.off()
# }

```




## 5 fit models
### A a little prep
```{r}
# minor prep
build_bin <- build %>% 
  st_drop_geometry() %>% 
  select(damage_binary, starts_with('build'), starts_with('parcel'),
         -build_has_tree_overhang) %>% 
  select(-starts_with('parcel_Effective')) %>% 
  filter(!is.na(build_Meandistroad),
         !is.na(build_dist_to_build_ft)) %>% glimpse() # strangly glimpse doesn't store the glimpse

# names(build_rforest_bin)

#map(build_bin, ~sum(is.na(.))) # check for NA's
apply(is.na(build_bin), 2, sum) # counts the NAs per col

summary(build_bin)

# double checks
build_bin %>% tabyl(damage_binary) # is this bad?
```



### B glm REMOVE? Demote?
```{r}

build_bin_MM_temp <- build %>% 
  st_drop_geometry() %>% 
  select(damage_binary, 
         build_treecan10, #build_grass10, build_shrub_10,
         parcel_EffectiveYear1, 
         build_Meanelev_30m,
         build_Meanslope_30m_DEM,
         build_Meanaspect_30m_DEM,
         build_Meandistroad,
         build_Meanbuilddens,
         build_Meandistall_road) %>% 
  drop_na() %>% 
  glimpse() # strangly glimpse doesn't store the glimpse

# names(build_rforest_bin)

updated_mod_20200924_MM  <- glm(damage_binary ~ build_treecan10 + # build_grass10 + build_shrub_10 +
                                  parcel_EffectiveYear1 + build_Meanelev_30m + build_Meanslope_30m_DEM +
                                  build_Meanaspect_30m_DEM + build_Meandistroad + build_Meanbuilddens +
                                  build_Meandistall_road,
                                data = build_bin_MM_temp, family = 'binomial')


performance::check_model(updated_mod_20200924_MM)

tab_model(updated_mod_20200924_MM)
         # file = paste0(getwd(),
         #               '/output_data/veg_logistic_reg_',
         #               gsub('[[:punct:]]', '_', Sys.time()), '.html'))
```



### B logistic regression
#### vegetation
```{r}
# build_bin_sel <- build_bin %>% select(damage_binary, 
#                                       build_treecan10 : build_shrub_300)

build_bin <- build %>% 
  st_drop_geometry() %>% 
  select(damage_binary, starts_with('build'), starts_with('parcel'),
         -build_has_tree_overhang)

map(build_bin, ~sum(is.na(.))) # check for NA's
# TREE
log_build_treecan10  <- glm(damage_binary ~ build_treecan10, data = build_bin, family = 'binomial')
log_build_treecan100 <- glm(damage_binary ~ build_treecan100,data = build_bin, family = 'binomial')
log_build_treecan200 <- glm(damage_binary ~ build_treecan200,data = build_bin, family = 'binomial')
log_build_treecan300 <- glm(damage_binary ~ build_treecan300,data = build_bin, family = 'binomial')

# performance::compare_performance(
#   log_build_treecan10, log_build_treecan100, log_build_treecan200, log_build_treecan300,
#   rank = TRUE)

# performance::check_model(log_build_treecan100)
#tab_model(log_build_treecan100)
# tab_model(log_build_treecan10, log_build_treecan100, log_build_treecan200, log_build_treecan300,
#           show.aic = TRUE, show.dev = TRUE)

# https://rdrr.io/rforge/modEvA/man/Dsquared.html
modEvA::Dsquared(log_build_treecan10)
modEvA::Dsquared(log_build_treecan100) # suggests 100
modEvA::Dsquared(log_build_treecan200)
modEvA::Dsquared(log_build_treecan300) 

# GRASS
log_build_grass10  <- glm(damage_binary ~ build_grass10, data = build_bin, family = 'binomial')
log_build_grass100 <- glm(damage_binary ~ build_grass_100,data = build_bin, family = 'binomial')
log_build_grass200 <- glm(damage_binary ~ build_grass_200,data = build_bin, family = 'binomial')
log_build_grass300 <- glm(damage_binary ~ build_grass_300,data = build_bin, family = 'binomial')

performance::compare_performance(
  log_build_grass10, log_build_grass100, log_build_grass200, log_build_grass300,
  rank = TRUE)

#performance::check_model(log_build_grass300)
#tab_model(log_build_grass300)

modEvA::Dsquared(log_build_grass10)
modEvA::Dsquared(log_build_grass100)
modEvA::Dsquared(log_build_grass200)
modEvA::Dsquared(log_build_grass300) # suggests 300


# SHRUB
log_build_shrub10  <- glm(damage_binary ~ build_shrub_10, data = build_bin, family = 'binomial')
log_build_shrub100 <- glm(damage_binary ~ build_shrub_100,data = build_bin, family = 'binomial')
log_build_shrub200 <- glm(damage_binary ~ build_shrub_200,data = build_bin, family = 'binomial')
log_build_shrub300 <- glm(damage_binary ~ build_shrub_300,data = build_bin, family = 'binomial')

# performance::compare_performance(
#   log_build_shrub10, log_build_shrub100, log_build_shrub200, log_build_shrub300,
#   rank = TRUE)
# 
# performance::check_model(log_build_shrub300)

modEvA::Dsquared(log_build_shrub10)
modEvA::Dsquared(log_build_shrub100)
modEvA::Dsquared(log_build_shrub200)
modEvA::Dsquared(log_build_shrub300) # suggests 300

# tab_model(log_build_treecan100, log_build_grass300, log_build_shrub300,
#          file = paste0(getwd(),
#                        '/output_data/veg_logistic_reg_',
#                        gsub('[[:punct:]]', '_', Sys.time()), '.html'))
```


#### other land covers
```{r}
# SOIL
log_build_soil10  <- glm(damage_binary ~ build_soil_10, data = build_bin, family = 'binomial')
log_build_soil100 <- glm(damage_binary ~ build_soil_100,data = build_bin, family = 'binomial')
log_build_soil200 <- glm(damage_binary ~ build_soil_200,data = build_bin, family = 'binomial')
log_build_soil300 <- glm(damage_binary ~ build_soil_300,data = build_bin, family = 'binomial')

# https://rdrr.io/rforge/modEvA/man/Dsquared.html
modEvA::Dsquared(log_build_soil10)
modEvA::Dsquared(log_build_soil100)
modEvA::Dsquared(log_build_soil200)
modEvA::Dsquared(log_build_soil300) # suggests 300

# WATER
log_build_water10  <- glm(damage_binary ~ build_water_10, data = build_bin, family = 'binomial')
log_build_water100 <- glm(damage_binary ~ build_water_100,data = build_bin, family = 'binomial')
log_build_water200 <- glm(damage_binary ~ build_water_200,data = build_bin, family = 'binomial')
log_build_water300 <- glm(damage_binary ~ build_water_300,data = build_bin, family = 'binomial')

# https://rdrr.io/rforge/modEvA/man/Dsquared.html
modEvA::Dsquared(log_build_water10)
modEvA::Dsquared(log_build_water100)
modEvA::Dsquared(log_build_water200)
modEvA::Dsquared(log_build_water300) # suggests 300


# BUILD
log_build_build10  <- glm(damage_binary ~ build_build_10, data = build_bin, family = 'binomial')
log_build_build100 <- glm(damage_binary ~ build_build_100,data = build_bin, family = 'binomial')
log_build_build200 <- glm(damage_binary ~ build_build_200,data = build_bin, family = 'binomial')
log_build_build300 <- glm(damage_binary ~ build_build_300,data = build_bin, family = 'binomial')

# https://rdrr.io/rforge/modEvA/man/Dsquared.html
modEvA::Dsquared(log_build_build10)
modEvA::Dsquared(log_build_build100)
modEvA::Dsquared(log_build_build200)
modEvA::Dsquared(log_build_build300) # suggests 300




# OTHER IMP
log_build_oth_imp10  <- glm(damage_binary ~ build_oth_imp_10, data = build_bin, family = 'binomial')
log_build_oth_imp100 <- glm(damage_binary ~ build_oth_imp_100,data = build_bin, family = 'binomial')
log_build_oth_imp200 <- glm(damage_binary ~ build_oth_imp_200,data = build_bin, family = 'binomial')
log_build_oth_imp300 <- glm(damage_binary ~ build_oth_imp_300,data = build_bin, family = 'binomial')

# https://rdrr.io/rforge/modEvA/man/Dsquared.html
modEvA::Dsquared(log_build_oth_imp10)
modEvA::Dsquared(log_build_oth_imp100)
modEvA::Dsquared(log_build_oth_imp200)
modEvA::Dsquared(log_build_oth_imp300) # suggests 300



# ROAD
log_build_road10  <- glm(damage_binary ~ build_road_10, data = build_bin, family = 'binomial')
log_build_road100 <- glm(damage_binary ~ build_road_100,data = build_bin, family = 'binomial')
log_build_road200 <- glm(damage_binary ~ build_road_200,data = build_bin, family = 'binomial')
log_build_road300 <- glm(damage_binary ~ build_road_300,data = build_bin, family = 'binomial')

# https://rdrr.io/rforge/modEvA/man/Dsquared.html
modEvA::Dsquared(log_build_road10)
modEvA::Dsquared(log_build_road100)
modEvA::Dsquared(log_build_road200)
modEvA::Dsquared(log_build_road300) # suggests 300


```


#### 'landscape position'
```{r}

log_build_Meanelev_30m  <- glm(damage_binary ~ build_Meanelev_30m,
                               data = build_bin, family = 'binomial')

log_build_Meanelev_100m <- glm(damage_binary ~ build_Meanelev_100m,
                               data = build_bin, family = 'binomial')

modEvA::Dsquared(log_build_Meanelev_30m) # suggests 30m
modEvA::Dsquared(log_build_Meanelev_100m)


log_build_Meanslope_30m_DEM <- glm(damage_binary ~ build_Meanslope_30m_DEM,
                                   data = build_bin, family = 'binomial')

log_build_Meanslope_100m_DEM <- glm(damage_binary ~ build_Meanslope_100m_DEM,
                              data = build_bin, family = 'binomial')

modEvA::Dsquared(log_build_Meanslope_30m_DEM) 
modEvA::Dsquared(log_build_Meanslope_100m_DEM)# suggests 100m


log_build_Meanaspect_30m_DEM <- glm(damage_binary ~ build_Meanaspect_30m_DEM,
                                   data = build_bin, family = 'binomial')

log_build_Meanaspect_100m_DEM <- glm(damage_binary ~ build_Meanaspect_100m_DEM,
                              data = build_bin, family = 'binomial')

modEvA::Dsquared(log_build_Meanaspect_30m_DEM)
modEvA::Dsquared(log_build_Meanaspect_100m_DEM) # suggests 100m


log_build_Meandistroad <- glm(damage_binary ~ build_Meandistroad,
                              data = build_bin, family = 'binomial')

log_build_Meanbuilddens <- glm(damage_binary ~ build_Meanbuilddens,
                              data = build_bin, family = 'binomial')

log_build_Meandistall_road <- glm(damage_binary ~ build_Meandistall_road,
                              data = build_bin, family = 'binomial')



# performance::compare_performance(
# log_build_Meanelev_30m,
# log_build_Meanelev_100m, 
# 
# log_build_Meanslope_30m_DEM, 
# log_build_Meanslope_100m_DEM, 
# 
# 
# log_build_Meanaspect_30m_DEM,
# log_build_Meanaspect_100m_DEM, 
# 
# log_build_Meandistroad,
# log_build_Meanbuilddens, 
# 
# 
# log_build_Meandistall_road, rank = TRUE)
# 
# 
# tab_model(log_build_Meanelev_30m,
#           log_build_Meanelev_100m, 
# 
#           log_build_Meanslope_30m_DEM, 
#           log_build_Meanslope_100m_DEM, 
#           
#           
#           log_build_Meanaspect_30m_DEM,
#           log_build_Meanaspect_100m_DEM, 
#           
#           log_build_Meandistroad,
#           log_build_Meanbuilddens, 
#           
#           log_build_Meandistall_road, show.aic = TRUE)
#  
# 
# 
# tab_model(log_build_Meanelev_30m,
#           log_build_Meanelev_100m, 
# 
#           log_build_Meanslope_30m_DEM, 
#           log_build_Meanslope_100m_DEM, 
#           
#           
#           log_build_Meanaspect_30m_DEM,
#           log_build_Meanaspect_100m_DEM, 
#           
#           log_build_Meandistroad,
#           log_build_Meanbuilddens, 
#           
#           log_build_Meandistall_road,
#          file = paste0(getwd(),
#                        '/output_data/landscape_logistic_reg_',
#                        gsub('[[:punct:]]', '_', Sys.time()), '.html'))

# # a very bad model
# log_r_1 <- glm(damage_binary~., data = build_bin, family = 'binomial')
# summary(log_r_1)
# # plot_model(log_r_1, sort.est = TRUE, vline.color = 'black') %>% theme_bw(16)
# # tab_model(log_r_1, digits = 6)
# 
# ptm <- proc.time() # clock in
# log_r_step <- glm(damage_binary~., data = build_rforest_bin, family = binomial) %>% 
#   MASS::stepAIC(trace = FALSE)
# (proc.time() - ptm) / 60 # clock out, about 12 minutes
# beepr::beep()
# plot_model(log_r_step, sort.est = TRUE, vline.color = 'black') %>% theme_bw(16)
# 
# # logistic regression validation
# # http://www.sthda.com/english/articles/36-classification-methods-essentials/150-stepwise-logistic-regression-essentials-in-r/
# # Make predictions
# # probabilities <- log_r_1 %>% predict(test.data, type = "response")
# # predicted.classes <- ifelse(probabilities > 0.5, "pos", "neg")
# # # Model accuracy
# # mean(predicted.classes==test.data$diabetes)

```

#### what did we learn from these logisitc regressions?
```{r}

tab_model(log_build_treecan100, log_build_grass300, log_build_shrub300, # veg
          log_build_soil300, log_build_water300, log_build_build300, log_build_oth_imp300, log_build_road300,
          log_build_Meanelev_30m, log_build_Meanslope_100m_DEM, log_build_Meanaspect_100m_DEM,
          log_build_Meandistroad, log_build_Meanbuilddens, log_build_Meandistall_road)

build_bin %>% select(build_treecan100,
                     build_grass_300,
                     build_shrub_300,
                     build_soil_300,
                     build_water_300,
                     build_build_300,
                     build_oth_imp_300,
                     build_road_300,
                     build_Meanelev_30m,
                     build_Meanslope_100m_DEM,
                     build_Meanaspect_100m_DEM,
                     build_Meandistroad,
                     build_Meanbuilddens,
                     build_Meandistall_road) %>% corr.test() -> corrs
corrs$r
 
build_bin %>% select(build_treecan100,
                     build_grass_300,
                     build_shrub_300,
                     build_soil_300,
                     build_water_300,
                     build_build_300,
                     build_oth_imp_300,
                     build_road_300,
                     build_Meanelev_30m,
                     build_Meanslope_100m_DEM,
                     build_Meanaspect_100m_DEM,
                     build_Meandistroad,
                     build_Meanbuilddens,
                     build_Meandistall_road) %>%
  sjt.corr(.,
         corr.method = 'spearman',
         triangle = 'lower',
         p.numeric = TRUE) #   file = paste0(getwd(), '/output_data/tables/cbg_demo_corrs.html'))

# build_bin %>% 
#   widyr::pairwise_cor(build_treecan100,
#                      build_grass_300,
#                      build_shrub_300,
#                      build_soil_300,
#                      build_water_300,
#                      build_build_300,
#                      build_oth_imp_300,
#                      build_road_300,
#                      build_Meanelev_30m,
#                      build_Meanslope_100m_DEM,
#                      build_Meanaspect_100m_DEM,
#                      build_Meandistroad,
#                      build_Meanbuilddens,
#                      build_Meandistall_road, upper = FALSE)
# 
# 

```


```{r}
build_bin_parsimony <- build_bin %>%
  select(damage_binary,
         build_treecan100, build_grass_300, build_shrub_300,
         build_soil_300, build_water_300, build_build_300, build_oth_imp_300, build_road_300,
         build_Meanelev_30m, build_Meanslope_100m_DEM, build_Meanaspect_100m_DEM,
         build_Meandistroad,
         build_Meanbuilddens, build_Meandistall_road) %>% 
  drop_na()
  
log_r_maximal <- glm(damage_binary~., data = build_bin_parsimony, family = binomial) 

log_r_step <- glm(damage_binary~., data = build_bin_parsimony, family = binomial) %>%
  MASS::stepAIC(trace = FALSE)

tab_model(log_r_maximal, log_r_step,
          show.aic = TRUE, show.r2 = TRUE)

modEvA::Dsquared(log_r_maximal)
modEvA::Dsquared(log_r_step)

performance::compare_performance(log_r_maximal, log_r_step)
```




### D fit Jeffrey Evan's worked example: randomForest
```{r}
# modeled after and adapted from this page
# from https://evansmurphy.wixsite.com/evansspatial/random-forest-sdm

# abbridged from J Evan's second chunk (chunk 1 accesses data)
library(randomForest)
library(rfUtilities)

b <- 1001                                     # Number of Bootstrap replicates
# snip
# chunks 3 and 4 import raster dadta and extract variables from raster to absence/present point locations
# snip

build_bin_parsimony

build_bin_parsimony %>% tabyl(damage_binary)

build_bin_parsimony %<>% mutate(damage_binary = as.factor(damage_binary)) %>% data.frame()# PAY ATTENTION HERE
build_bin_parsimony %>% glimpse


(cl <- multi.collinear(build_bin_parsimony[,2:ncol(build_bin_parsimony)], p = 0.05))

# tell us what's got to go!
# for(l in cl) {
#   cl.test <- build_bin_parsimony[,-which(names(build_bin_parsimony)==l)]
#   print(paste("Remove variable", l, sep=": "))
#   multi.collinear(cl.test, p=0.05) 
# }
# 
# 
# build_bin_parsimony <- build_bin_parsimony[,-which(names(build_bin_parsimony) %in% cl )] %>% 
#   data.frame()

# Chunk 8 makes depenent variable categorical (done). BUT it also checks for frequency of presense

# "We observe that the sample balance of presence locations is 33% thus, meeting the 1/3 rule for sample balance."

build_bin_parsimony %>% tabyl(damage_binary)
print('12% - is this a problem?!?!')

# model selection
# ( rf.model <- rf.modelSel(x=sdata@data[,3:ncol(sdata@data)], y=sdata@data[,"Present"], imp.scale="mir", ntree=b) )

ptm <- proc.time() # clock in
(rf.model <- rf.modelSel(x=build_bin_parsimony[,2:ncol(build_bin_parsimony)],
                         y=build_bin_parsimony[,"damage_binary"],
                         imp.scale="mir", ntree=b))
(proc.time() - ptm) / 60 # clock out, about 3 minutes
beepr::beep()


sel.vars <- rf.model$selvars

# OR, pick a model?
#sel.vars <- rf.model$parameters[[4]] # DECISION POINT

# run a model
ptm <- proc.time() # clock in
( rf.fit <- randomForest(y=build_bin_parsimony[,"damage_binary"], x=build_bin_parsimony[,sel.vars],
                         ntree=b,
                         importance=TRUE, norm.votes=TRUE, proximity=TRUE) )
(proc.time() - ptm) / 60 # clock out, about 3 minutes
beepr::beep()


# run a model with ALL PREDICTORS
ptm <- proc.time() # clock in
( rf.fit_all <- randomForest(y=build_bin_parsimony[,"damage_binary"],
                             x=build_bin_parsimony[,2:ncol(build_bin_parsimony)],
                         ntree=b,
                         importance=TRUE, norm.votes=TRUE, proximity=TRUE) )
(proc.time() - ptm) / 60 # clock out, about 4 minutes
beepr::beep()


ptm <- proc.time() # clock in
( imbal <- randomForestSRC::imbalanced(damage_binary~., data=build_bin_parsimony) )
(proc.time() - ptm) / 60 # clock out, about 3 minutes
beepr::beep()

# skipping chunks 13 and 14: predicted raster map.. doesn't really apply here.

# model fit!
rf.pred <- predict(rf.fit, build_bin_parsimony[,sel.vars], type="response")

rf.prob <- as.data.frame(predict(rf.fit, build_bin_parsimony[,sel.vars], type="prob"))

obs.pred <- data.frame(cbind(Observed=as.numeric(as.character(build_bin_parsimony[,"damage_binary"])),
                             PRED=as.numeric(as.character(rf.pred)), Prob1=rf.prob[,2],
                             Prob0=rf.prob[,1]) )

op <- (obs.pred$Observed == obs.pred$PRED)



( pcc <- (length(op[op == "TRUE"]) / length(op))*100 )



library(verification)

roc.plot(obs.pred[,"Observed"], obs.pred[,"Prob1"])


# model validation
ptm <- proc.time() # clock in
( rf.perm <- rf.significance(rf.fit, build_bin_parsimony[,sel.vars], nperm = 99, ntree = 1001) )
(proc.time() - ptm) / 60 # clock out, 59 minutes with 99 permutations
beepr::beep()

saveRDS(rf.perm, file = "output_data/rf.perm.rds")

# cross validation
ptm <- proc.time() # clock in
( rf.cv <- rf.crossValidation(rf.fit, build_bin_parsimony[,sel.vars], p=0.10, n=99, ntree=1001) )
(proc.time() - ptm) / 60 # clock out, about 143 minutes (2.38 hours) with 999 permutations
beepr::beep()

# so this model isn't very good

saveRDS(rf.cv, file = "output_data/rf.perm.rds")

# so the number of trees is probably over kill at 1000? 75 sufficient?
plot(rf.fit, main="Bootstrap Error Convergence")


# variable importance
p <- as.matrix(rf.fit$importance[,3])   
ord <- rev(order(p[,1], decreasing=TRUE)[1:dim(p)[1]]) 


png(file = paste0(getwd(), '/figures/var_imp', gsub('[[:punct:]]', '_', Sys.time()), '.png'))
dotchart(p[ord,1], main="Scaled Variable Importance", pch=19)  
dev.off()


p <- as.matrix(rf.fit_all$importance[,3])   
ord <- rev(order(p[,1], decreasing=TRUE)[1:dim(p)[1]]) 


png(file = paste0(getwd(), '/figures/var_imp_all', gsub('[[:punct:]]', '_', Sys.time()), '.png'))
dotchart(p[ord,1], main="Scaled Variable Importance", pch=19)  
dev.off()

(all_vars <- names(build_bin_parsimony))
par(mfrow=c(1,1))
for(i in all_vars[2:15]) {
  png(file = paste0(getwd(), '/figures/', i, '_',
                  gsub('[[:punct:]]', '_', Sys.time()), '.png'))
  rf.partial.prob(rf.fit_all, build_bin_parsimony[,all_vars], i, "1", smooth="spline", raw.line=FALSE)
  dev.off()
}  



# par(mfrow=c(2,2))
# for(i in sel.vars[1:4]) {
#   rf.partial.prob(rf.fit, build_rforest_cat[,sel.vars], i, "1", smooth="spline", raw.line=FALSE)
# }  


for(i in sel.vars) {
png(file = paste0(getwd(), '/figures/', i, '_',
                  gsub('[[:punct:]]', '_', Sys.time()), '.png'))
rf.partial.prob(rf.fit, build_bin_parsimony[,sel.vars], i, "1", smooth="spline", raw.line=FALSE)
dev.off()
  }  
beepr::beep()
```






### C R-part and cforest
```{r}

# recursive partitioning
rpart_1 <- rpart::rpart(damage_binary~.,data = build_bin)#, cp = 0.02)
summary(rpart_1)


png(file = paste0(getwd(), '/figures/rpart_', gsub('[[:punct:]]', '_', Sys.time()), '.png'))
rpart.plot::rpart.plot(rpart_1)
dev.off()
#labels(rpart_1)

# ptm <- proc.time() # clock in
# party_1 <- party::cforest(damage_binary~., data = build_bin)
# party_1_imp <- varimp(party_1); View(party_1_imp)
# (proc.time() - ptm) / 60 # clock out, about 6 minutes
# beepr::beep()
# 
# 
# cf <- party_1
# pt <- prettytree(cf@ensemble[[1]], names(cf@data@get("input"))) 
# nt <- new("BinaryTree") 
# nt@tree <- pt 
# nt@data <- cf@data 
# nt@responses <- cf@responses 

#plot(nt, type="simple")
```



#### i random forest readings

Thanks Sebastian Martinuzzi

https://evansmurphy.wixsite.com/evansspatial/random-forest-sdm

https://cran.r-project.org/web/packages/rfUtilities/index.html
(Utilities for Random Forest model selection, class balance correction, significance test, cross validation and partial dependency plots.)

https://www.rdocumentation.org/packages/randomForestSRC/versions/2.9.3/topics/imbalanced.rfsrc
(Imbalanced Two Class Problems)

Old, but important
https://statistics.berkeley.edu/sites/default/files/tech-reports/666.pdf

A review paper on "A survey on addressing high-class imbalance in big data"
https://link.springer.com/article/10.1186/s40537-018-0151-6

https://towardsdatascience.com/random-forest-in-r-f66adf80ec9

http://www.sthda.com/english/articles/36-classification-methods-essentials/150-stepwise-logistic-regression-essentials-in-r/#:~:text=The%20stepwise%20logistic%20regression%20can,ref(stepwise%2Dregression))

https://compstat-lmu.github.io/iml_methods_limitations/pdp.html

https://www.blopig.com/blog/2017/04/a-very-basic-introduction-to-random-forests-using-r/




### D fit Jeffrey Evan's worked example: randomForest
```{r}
# modeled after and adapted from this page
# from https://evansmurphy.wixsite.com/evansspatial/random-forest-sdm

# abbridged from J Evan's second chunk (chunk 1 accesses data)
library(randomForest)
library(rfUtilities)

b <- 1001                                     # Number of Bootstrap replicates
# snip
# chunks 3 and 4 import raster dadta and extract variables from raster to absence/present point locations
# snip


# # minor prep
# build_rforest_cat <- build %>% 
#   st_drop_geometry() %>% 
#   select(damage_binary, starts_with('build'), starts_with('parcel'),
#          -build_has_tree_overhang) %>% 
#   mutate(damage_binary = as.factor(damage_binary)) %>% # PAY ATTENTION HERE
#   filter(!is.na(build_Meandistroad),
#          !is.na(build_dist_to_build_ft))
# 
# build_rforest_cat %>% tabyl(damage_binary)

build_bin %>% tabyl(damage_binary)

# cl <- multi.collinear(sdata@data[,3:ncol(sdata@data)], p=0.05) # the example
# cl <- multi.collinear(build_rforest_cat[,2:ncol(build_rforest_cat)], p = 0.05) # earlier version

build_bin %<>% mutate(damage_binary = as.factor(damage_binary)) # PAY ATTENTION HERE
build_bin %>% glimpse

cl <- multi.collinear(build_bin[,2:ncol(build_bin)], p = 0.05)

# tell us what's got to go!
for(l in cl) {
  cl.test <- build_bin[,-which(names(build_bin)==l)]
  print(paste("Remove variable", l, sep=": "))
  multi.collinear(cl.test, p=0.05) 
}

# make the removals
# build_rforest_cat <- build_rforest_cat[,-which(names(build_rforest_cat) %in% cl )] %>% 
#   data.frame()

build_bin <- build_bin[,-which(names(build_bin) %in% cl )] %>% 
  data.frame()

# Chunk 8 makes depenent variable categorical (done). BUT it also checks for frequency of presense

# "We observe that the sample balance of presence locations is 33% thus, meeting the 1/3 rule for sample balance."

build_bin %>% tabyl(damage_binary)
print('12% - is this a problem?!?!')

# model selection
# ( rf.model <- rf.modelSel(x=sdata@data[,3:ncol(sdata@data)], y=sdata@data[,"Present"], imp.scale="mir", ntree=b) )

ptm <- proc.time() # clock in
(rf.model <- rf.modelSel(x=build_bin[,2:ncol(build_bin)],
                         y=build_bin[,"damage_binary"],
                         imp.scale="mir", ntree=b))
(proc.time() - ptm) / 60 # clock out, about 3 minutes
beepr::beep()


sel.vars <- rf.model$selvars

# OR, pick a model?
#sel.vars <- rf.model$parameters[[4]] # DECISION POINT

# run a model
ptm <- proc.time() # clock in
( rf.fit <- randomForest(y=build_bin[,"damage_binary"], x=build_bin[,sel.vars],
                         ntree=b,
                         importance=TRUE, norm.votes=TRUE, proximity=TRUE) )
(proc.time() - ptm) / 60 # clock out, about 3 minutes
beepr::beep()


# run a model with ALL PREDICTORS
ptm <- proc.time() # clock in
( rf.fit_all <- randomForest(y=build_bin[,"damage_binary"],
                             x=build_bin[,2:ncol(build_bin)],
                         ntree=b,
                         importance=TRUE, norm.votes=TRUE, proximity=TRUE) )
(proc.time() - ptm) / 60 # clock out, about 4 minutes
beepr::beep()




ptm <- proc.time() # clock in
( imbal <- randomForestSRC::imbalanced(damage_binary~., data=build_bin) )
(proc.time() - ptm) / 60 # clock out, about 3 minutes
beepr::beep()

# skipping chunks 13 and 14: predicted raster map.. doesn't really apply here.

# model fit!
rf.pred <- predict(rf.fit, build_bin[,sel.vars], type="response")

rf.prob <- as.data.frame(predict(rf.fit, build_bin[,sel.vars], type="prob"))

obs.pred <- data.frame(cbind(Observed=as.numeric(as.character(build_bin[,"damage_binary"])),
                             PRED=as.numeric(as.character(rf.pred)), Prob1=rf.prob[,2],
                             Prob0=rf.prob[,1]) )

op <- (obs.pred$Observed == obs.pred$PRED)



( pcc <- (length(op[op == "TRUE"]) / length(op))*100 )



library(verification)

roc.plot(obs.pred[,"Observed"], obs.pred[,"Prob1"])


# model validation
ptm <- proc.time() # clock in
( rf.perm <- rf.significance(rf.fit, build_bin[,sel.vars], nperm = 99, ntree = 1001) )
(proc.time() - ptm) / 60 # clock out, 59 minutes with 99 permutations
beepr::beep()

saveRDS(rf.perm, file = "output_data/rf.perm.rds")

# cross validation
ptm <- proc.time() # clock in
( rf.cv <- rf.crossValidation(rf.fit, build_bin[,sel.vars], p=0.10, n=99, ntree=1001) )
(proc.time() - ptm) / 60 # clock out, about 143 minutes (2.38 hours) with 999 permutations
beepr::beep()

# so this model isn't very good

saveRDS(rf.cv, file = "output_data/rf.perm.rds")

# so the number of trees is probably over kill at 1000? 75 sufficient?
plot(rf.fit, main="Bootstrap Error Convergence")


# variable importance
p <- as.matrix(rf.fit$importance[,3])   
ord <- rev(order(p[,1], decreasing=TRUE)[1:dim(p)[1]]) 


png(file = paste0(getwd(), '/figures/var_imp', gsub('[[:punct:]]', '_', Sys.time()), '.png'))
dotchart(p[ord,1], main="Scaled Variable Importance", pch=19)  
dev.off()


p <- as.matrix(rf.fit_all$importance[,3])   
ord <- rev(order(p[,1], decreasing=TRUE)[1:dim(p)[1]]) 


png(file = paste0(getwd(), '/figures/var_imp_all', gsub('[[:punct:]]', '_', Sys.time()), '.png'))
dotchart(p[ord,1], main="Scaled Variable Importance", pch=19)  
dev.off()

all_vars <- names(build_bin)
par(mfrow=c(1,1))
for(i in all_vars[2:52]) {
  png(file = paste0(getwd(), '/figures/', i, '_',
                  gsub('[[:punct:]]', '_', Sys.time()), '.png'))
  rf.partial.prob(rf.fit_all, build_rforest_cat[,all_vars], i, "1", smooth="spline", raw.line=FALSE)
  dev.off()
}  



# par(mfrow=c(2,2))
# for(i in sel.vars[1:4]) {
#   rf.partial.prob(rf.fit, build_rforest_cat[,sel.vars], i, "1", smooth="spline", raw.line=FALSE)
# }  


for(i in sel.vars) {
png(file = paste0(getwd(), '/figures/', i, '_',
                  gsub('[[:punct:]]', '_', Sys.time()), '.png'))
rf.partial.prob(rf.fit, build_bin[,sel.vars], i, "1", smooth="spline", raw.line=FALSE)
dev.off()
  }  
beepr::beep()
```



## 2 bring in parcels
### 2a polygons
```{r eval=FALSE, include=FALSE}
parcels_sf <- read_sf('data/WoosleyFire_eCogOuput_06Dec2019/Parcels/1.shp') %>% 
  st_transform(crs = st_crs(build_sf)) %>% # step even needed
  mutate(area_from_st = st_area(.))

parcels_sf
```

### 2b
```{r eval=FALSE, include=FALSE}
data_path <- 'data/WoosleyFire_eCogOuput_06Dec2019/LandCover_per_Parcel'; data_path # building dist build
files <- dir(data_path, recursive = TRUE, pattern = "*.csv"); files # get file names

parcel_land_cover <-
  tibble(filename = files) %>% # create a data frame, holding the file names
  mutate(file_contents = map(filename,  # read files into
                             ~ read.csv(file.path(data_path, .),
                                        fileEncoding='latin1',
                                        colClasses = c('character'),
                                        sep = ';'))) %>%  # a new data column
  unnest() %>%
  clean_names() %>%
  mutate_at(vars(starts_with('rel_')), as.numeric) %>% 
  mutate_at(vars(starts_with('rel_')), list(~. * 100)) %>% 
  select(ID_Parcel = id_parcel_parcels,
         area_ft_survey,
         pct_tree_can = rel_area_of_sub_objects_tree_canopy_1,
         pct_grass = rel_area_of_sub_objects_grass_1,
         pct_soil = rel_area_of_sub_objects_bare_soil_1,
         pct_water = rel_area_of_sub_objects_water_1,
         pct_build = rel_area_of_sub_objects_buildings_1,
         pct_road = rel_area_of_sub_objects_roads_1,
         pct_oth_imp = rel_area_of_sub_objects_other_impervious_1,
         pct_shrub = rel_area_of_sub_objects_shrub_1) %>% 
  filter(ID_Parcel != 'undefined') %>% 
  mutate(ID_Parcel = as.numeric(ID_Parcel)) %>% 
  arrange(ID_Parcel) %>% 
  group_by(ID_Parcel) %>% 
  arrange(desc(pct_tree_can),
          desc(pct_grass),
          desc(pct_soil),
          desc(pct_water),
          desc(pct_build),
          desc(pct_road),
          desc(pct_oth_imp),
          desc(pct_shrub), .by_group = TRUE) %>% 
  slice(1) %>%                  # grabs the first record
  ungroup()

# parcel_land_cover # FIXME TOO LONG
# parcel_land_cover %>% 
#   group_by(ID_Parcel) %>% 
#   count() %>% 
#   arrange(desc(n)) -> dup_parcel_land_cover

parcels_sf %>% 
  left_join(parcel_land_cover,
            by = c('ID_Parcel' = 'ID_Parcel'))

```

## 3 bring in ACS data
```{r}
acs_2018 <- load_variables(2018, 'acs5', cache = FALSE); View(acs_2018)
# set mapping options
# this makes loading the data faster on the second/subsquent version.
options(tigris_use_cache = TRUE)

# read in year 2009 - 2013 5-year ACS
cbg_sf <- get_acs(state = 'CA',                # Connecticut
                  county = 'Los Angeles County', # larger than we want, hence filter below
                  geography = 'block group',   # blocks are within block groups, block groups are within tracts. Not all of the data we want are available in blocks
                  year = 2018,                 # matches line 26
                  survey = 'acs5',             # matches line 26
                  moe_level = 95,              # margin of error level, default is 90 I prefer the tighter 95
                  summary_var = 'B02001_001',  # see help(get_acs) and the tidycensus links above
                                               # this is total population
                  variables = c(medincome = 'B19013_001', # median household income, $
                                
                                white = 'B02001_002',     # Caucasian, counts
                                black = 'B02001_003',     # African American, counts
                                am_ind = 'B02001_004',    # Native American, counts
                                asian = 'B02001_005',     # Asian, counts
                                pac_is = 'B02001_006',    # Pacific Islander, counts
                                other_race ='B02001_007', # Other, counts
                                
                                # FIXME add ethnicity
                                # Pct hispanic (any race)
                                # percent white not-hispanic
                                
                                tot_hu = 'B25003_001',
                                own_occ_hu = 'B25003_002',
                                rent_occ_hu = 'B25003_003',
                                vacant = 'B25004_001',
                               
                                edu_base = 'B15002_001',
                                
                                edu_hs_m = 'B15002_011',
                                edu_sc_m = 'B15002_012',
                                edu_sc1m = 'B15002_013',
                                edu_aa_m = 'B15002_014',
                                edu_ba_m = 'B15002_015',
                                edu_ma_m = 'B15002_016',
                                edu_po_m = 'B15002_017',
                                edu_phdm = 'B15002_018',
                                
                                edu_hs_f = 'B15002_028',
                                edu_sc_f = 'B15002_029',
                                edu_sc1f = 'B15002_030',
                                edu_aa_f = 'B15002_031',
                                edu_ba_f = 'B15002_032',
                                edu_fa_f = 'B15002_033',
                                edu_po_f = 'B15002_034',
                                edu_phdf = 'B15002_035'),
                  output = 'wide',
                  geometry = TRUE) %>% # this creates the spatial data
  st_transform(crs = st_crs(build_sf)) %>% 
  mutate(tot_pop = summary_est,
         pct_white = (whiteE / summary_est)*100,
         pct_black = (blackE / summary_est)*100,
         pct_am_ind= (am_indE/ summary_est)*100,
         pct_asian = (asianE / summary_est)*100,
         pct_pac_is= (pac_isE / summary_est)*100,
         pct_other_r=(other_raceE/summary_est)*100,
         
         pct_own_occ=(own_occ_huE/tot_huE*100),
         pct_vac   = (vacantE / tot_huE*100),
         
         pct_edu = ((edu_hs_mE + edu_sc_mE + edu_sc1mE + edu_aa_mE + edu_ba_mE + edu_ma_mE + edu_po_mE + edu_phdmE +
                    edu_hs_fE + edu_sc_fE + edu_sc1fE + edu_aa_fE + edu_ba_fE + edu_fa_fE + edu_po_fE + edu_phdfE) /
                      edu_baseE) * 100) %>%
  select(GEOID, tot_pop, medincomeE, starts_with('pct_'))

# plot(cbg_sf) # looks right

# mapview(cbg_sf) + mapview(build_sf, col.region = 'red')

system.time(
  build_w_cbg <- build_sf %>%
  st_join(cbg_sf,           # spatial join
          left = FALSE,     # if FALSE, only intersection point will, be returned, instead all points returned, 
          largest = TRUE))  # and attributes of intersection
                            # largest = TRUE is AMAZING, but makes things much slower.
mapview(build_w_cbg)

# was the join 'clean' 1 to M?
dim(build_sf)
build_w_cbg %>% 
  st_drop_geometry() %>% 
  group_by(GEOID) %>% 
  count() %>% 
  arrange(desc(n)) -> test; sum(test$n)

build_w_cbg %>% 
  st_drop_geometry() %>% 
  group_by(ID_Build) %>% 
  count() %>% 
  arrange(desc(n)) -> test2; sum(test2$n)

# good join!
rm(test, test2)

build_w_cbg %>% 
  tabyl(GEOID) %>% 
  arrange(desc(n))
```





## 4 What's going on with the building data?
```{r}
build_sf

hist(build_sf$area)
```



```{r sandbox, eval=FALSE, include=FALSE}
# 
# csv2 <- read.csv('data/WoosleyFire_MappingBlocks_Output_v1/Building_dist_Building/2.csv', sep = ';') %>% View()
# 
# csv20 <- read.csv('data/WoosleyFire_MappingBlocks_Output_v1/Building_dist_Building/20.csv', sep = ';') %>% View()
# 
# csv6 <- read.csv('data/WoosleyFire_MappingBlocks_Output_v1/Building_dist_Building/6.csv', sep = ';') %>% View()
# 
# 
# 
# read_plus <- function(flnm) {
#     read_csv(flnm) %>% 
#         mutate(filename = flnm)
# }
# 
# tbl_with_sources <-
#     list.files(path = data_path, full.names = TRUE, pattern = "*.csv") %>% 
#     map_df(~read_plus(.)) %>% 
#   separate("inner_x;inner_y;level_name;class_name;Distance to Buildings (Pxl);\"ID_Build\"  Buildings",
#            into = c('x', 'y', 'level', 'dist', 'id', 'build'), sep = ';') %>% 
#   filter(build == '2637')
# 
#   
#   
#   
#   as_tibble() %>% 
#   clean_names() %>%
#   mutate(dist_to_build_ft = pxl_to_ft_conversion*as.numeric(distance_to_buildings_pxl)) %>% # throws and NA error, we want those NAs. OK!
#   select(ID_Build = id_build_buildings,
#          dist_to_build_ft) %>%
#   group_by(ID_Build) %>%           # used for degubbing, lots of duplicate ID_Build
#   add_tally(name = 'n_build_obs') %>% 
#   arrange(desc(n_build_obs), desc(dist_to_build_ft)) %>% # sorting for ease of debug
#   ungroup() %>% 
#   filter(ID_Build != 'undefined')                 
# 

#dat_csv <- plyr::ldply(paste(data_path, files, sep = '/'), read_csv)





build %>% left_join(st_drop_geometry(parcel_perim), by = c('ID_Parcel.x' = 'ID_Build')
```


```{r, citations}
lapply(packages, citation)
```


Last knit on `r format(Sys.time())`

## old/ sandbox
```{r}
#table(build_dins$Buildings_ParcelID_DINS_intersects_DAMAGE, useNA = 'ifany')
# test out the joins
build <- build_dins %>% left_join(st_drop_geometry(build_sf), by = c('ID_Build' = 'ID_Build'))


# read in the building polygons
build_sf <- read_sf(paste0(getwd(), '/data/Woosley_Buildings_ParcelID'), quiet = FALSE)
build_sf

build %>% left_join(st_drop_geometry(build_sf), by = c('ID_Build' = 'ID_Build'))


# is ID_Build unique for each row?
identical(length(unique(build_sf$ID_Build)), dim(build_sf)[1]) # yes

# what is the building to parcel relationship (1:M)
build_sf %>%
     st_drop_geometry() %>%    # droping spatial data for ease of reading
     group_by(ID_Parcel) %>%   # grouping data by thing
     count() %>%               # counting elements in the group
     arrange(desc(n)) -> build_per_parcel

plot(build_per_parcel$n, main = 'Are we cool with this?')
abline(h = mean(build_per_parcel$n), col = 'red')
abline(h = median(build_per_parcel$n), col = 'blue')
text(x = 3000, y = 125, labels = paste0('mean # buildings per parcel: ',
                                        round(mean(build_per_parcel$n), 3), '\n',
                                        'median # of buildings per parcel: ',
                                        median(build_per_parcel$n)))
# lets look at the map
# mapview(build_sf) # looks good

```



```{r}
system.time(save.image(file = paste0('saved_sessions/wui_r_', gsub('[[:punct:]]', '-', Sys.time()), '.RData')))
```

