---
title: "04_fit_models.Rmd"
author: "Dexter H. Locke, PhD"
date: "`r format(Sys.time())`"
output: html_document
editor_options: 
  chunk_output_type: console
---




```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

This assumes "02_Woosley_combine_data.Rmd" was run



## 0 set up: load libraries, custom functions, set defaults
```{r}
# load libraries
# packages we'll be using
packs <- c('tidyverse'        # a must have!
           , 'tidylog'        # makes things very verbose for 2x checking 
           , 'magrittr'       # all of the pipes
           , 'janitor'        # cleans things up
           # , 'sf'             # simple features
           # , 'mapview'        # quick webmaps for zoom/pan viz
           #'tidycensus',     # access to Census data in a tidy way
           #'party',          # random forests
           , 'modEvA'         # contains D-squared function
           , 'randomForest'  #'rfUtilities', 'verification'
           , 'tictoc'         # times things
           , 'beepr'          # makes noises
            ,  'ggpubr'        #for scatter plot
           # , 'broom'          # tidy up regression models
           # , 'performance'    # nice regression diagnostics
           # , 'psych'          # describe is very useful for descriptive statistics
           # , 'sjPlot'         # useful plotting and regression support
           # , 'rpart'          # for random forests
           # # , pROC           # for AUC calculation
           , 'pdp'            # partial dep plots
           , 'vip'            # Variable Importance Plots (and extracting vip vals)
           # , 'caret'          #random forest work
           , 'party'          # for random forests using cforest
           )        

# check for all of the libraries
if (length(setdiff(packs, rownames(installed.packages()))) > 0) {
  install.packages(setdiff(packs, rownames(installed.packages())))  
}

# lapply(packs, library, character.only = TRUE)
vapply(packs, library, character.only = TRUE, logical(1),
       logical.return = TRUE, quietly = TRUE)


# custom function for "Not In"
`%nin%` <- Negate(`%in%`)

# for reproducibility, we should have the same random draws. setting the seed ensures that is the case.
set.seed(19870630)
```



## 1 read in data
### A main `build` file
```{r}

# one 'main' data frame
build <- read_csv(paste0(getwd(), '/output_data/building_2022-02-01.csv')) %>%
  tidylog::select(
    # damage_binary
      damage_cat         # binary outcome encoded as categorical (Destroyed / Survived)
    , damage_severity    # five levels
    , build_area   
    # build_DECKPORCHE    , #drop this on 12/20/21
    , build_DECKPORCHO
    , build_EAVES
    , build_EXTERIORSI
    , build_FENCEATTAC
    , build_PATIOCOVER
    , build_PROPANETAN
    , build_ROOFCONSTR
    , build_VEGCLEARAN 
    , build_VENTSCREEN 
    , build_WINDOWPANE 
    , parcel_year_built 
    , build_near_build 
    , build_has_tree_overhang 
    , build_min_dist_shrub 
    , build_min_dist_tree 
    , build_overhang_ht 
    , build_perc_overhang 
    , parcel_Build_P 
    , parcel_Can_P 
    , parcel_Grass_P 
    #, parcel_Imperv_P    #inverse of perv surface; shld we just drop bc we don't have for other measures?
    , parcel_Paved_P    
    #  parcel_Perv_P    ,#inverse of imp, just need one
    , parcel_Road_P
    , parcel_Shrub_P  
    , parcel_Soil_P  
    , parcel_Water_P  
    , build_near_dest  
    , build_p_building_10  
    , build_p_building_100  
    , build_p_building_200  
    , build_p_building_300  
    , build_p_grass_10  
    , build_p_grass_100  
    , build_p_grass_200  
    , build_p_grass_300  
    , build_p_otherpaved_10  
    , build_p_otherpaved_100  
    , build_p_otherpaved_200  
    , build_p_otherpaved_300  
    , build_p_road_10  
    , build_p_road_100  
    , build_p_road_200  
    , build_p_road_300  
    , build_p_shrub_10  
    , build_p_shrub_100  
    , build_p_shrub_200  
    , build_p_shrub_300  
    , build_p_soil_10  
    , build_p_soil_100  
    , build_p_soil_200  
    , build_p_soil_300  
    , build_p_tree_10  
    , build_p_tree_100  
    , build_p_tree_200  
    , build_p_tree_300  
    , build_p_water_10  
    , build_p_water_100  
    , build_p_water_200  
    , build_p_water_300  
    , parcel_area  
    , build_Mean_builddens  
    , build_Mean_distall_road  
    , build_Mean_distroad  
    , build_Mean_aspect_100m_DEM  
    , build_Mean_aspect_30m_DEM  
    , build_Mean_elev_100m  
    , build_Mean_elev_30m  
    , build_Mean_slope_100m_DEM  
    , build_Mean_slope_30m_DEM
    , build_DINS
    ) %>%
  mutate_if(is.character, as.factor) %>% # turns characters into factors
  droplevels() %>% 
  data.frame() # some of the random forest functions do not 'like' tibbles
               # since the non-dins and dins data will be pulled from here
               # we change here so that the subsets are already formatted as data.frames

# Miranda: do these NA's surprise you? OK to drop non-DINS-associated NA's?
#  2/23/22 Dexter: As long as it does not also discard DINS observations. I don't see the harm in keeping them, given that we do have many NAs in DINS.
# where are the NA's
build %>% 
  map(~sum(is.na(.))) %>% # map is like for loop but vectorized, meaning its faster uses dif. syntax help(purrr::map)
  bind_rows() %>%         # the output of purrr::map is a list, this makes a tibble
  t()#  %>% View() # check for NA's ("t" is for Transpose, just makes it easier to read)

# Miranda, are these NA's expected? #  2/23/22Dexter, see line 159 above.
```



### B non-DINS data (larger dataset, fewer predictors)
```{r}

# a subset for non-DINS-associated data
build_no_dins <- build %>% 
  tidylog::select(# dropping columns containing specific DINS data
                  # very detailed but sparse (lots of NAs)
    !c(  build_DECKPORCHO
       , build_EAVES
       , build_EXTERIORSI
       , build_FENCEATTAC
       , build_PATIOCOVER
       , build_PROPANETAN
       , build_ROOFCONSTR
       , build_VEGCLEARAN
       , build_VENTSCREEN
       , build_WINDOWPANE
       , build_DINS
       , damage_severity # dropping the DV for the smaller model
       )
    )

# NAs are expected
build_no_dins %>% 
  map(~sum(is.na(.))) %>% 
  bind_rows() %>% 
  t() # %>% View() # check for NA's

# how is the outcome distributed?
build         %>% tabyl(damage_cat) # good double check
build_no_dins %>% tabyl(damage_cat)

```



### C DINS data (smaller dataset, more predictors)
```{r}
# get data for DINS analyses
build %>% tabyl(build_DINS)

# subset
build_dins <- build %>% 
  filter(build_DINS == 1) %>% 
  select(-build_DINS, -damage_cat)
  # dropping the DV for the smaller model

# Miranda, even after dropping the non-DINS data are there are a lot of NA's in the DINS columns
# do we need all of them?
# in other words, not even the DINS data are complete for DINS data
#  2/23/22 Dexter, please see line 195 above - these are expected.
build_dins %>% 
  map(~sum(is.na(.))) %>% 
  bind_rows() %>% 
  t() # %>% View() # check for NA's


# when we keep only complete cases (so remove all NA's) the file is really short (n = 172)
# is this a surprise?
#  2/23/22 Dexter, please see line 195 above - this is not a surprise.
dim(build_dins)
build_dins %>% drop_na() %>% dim() # not going to be practical for randomForest::randomForest

build_dins %>% tabyl(damage_severity)

```



## 2 fit random forest models
### A non-DINS analyses (larger dataset, fewer predictors)
```{r}
# fit random forest models
# randomForest::randomForest is faster but requires full data
# party::cforest is slower but can handle missing data

#n_tree <- 10 # used for setting number of trees
n_tree <- 2000 #3/24/2022 changed back to 2000 trees as Price paper uses 2000


# # random forest but has to drop NAs
# tic(); rf_no_dins <- randomForest::randomForest(
#   damage_cat ~ .
#   # need full dataset, can't keep NA's
#   , data = build_no_dins %>% drop_na()
#   , type = 'classification' # not regression
#   , mtry = 5
#   , ntree = n_tree # 10 ~.5 seconds,100 ~5s, 200 ~7.5s, 3 ~12s, ... 5 ~23s 
#                    # tested on a few smaller forests
#   ); toc()


# # since this takes a long time to run, I've commented the code off and just read-in
# # the completed model file in its "*.Rdata" format with the load() function.
# sounds good - I had to rerun this on my machine on 3/24/22 so will see that date below
# # random forest via party::cforest which accepts missing data and is slower 
# tic(); cf_no_dins <- party::cforest(
#   damage_cat ~ .
#   , data = build_no_dins # notice no drop_na here
#   , controls =
#     cforest_control(
#         mtry = 5
#       , ntree = n_tree # 10 ~2 seconds, 100 ~25s, 200 ~50s, 300 ~75s ... 500 ~130 seconds
#       , mincriterion = 0
#       )
#   ); toc()

# # save out since it is slow
# sounds good - I had to rerun this on my machine on 3/24/22 so will see that date below
#tic(); save(cf_no_dins
#             , file = paste0('../Massive_forests/cf_no_dins_', Sys.Date(), '.Rdata')); toc()

# read back in
#tic(); load(file = '../Massive_forests/cf_no_dins_2022-02-22.Rdata'); toc() # ~9s


# MHM - last date I ran this was 3/24/22 so will see that date below
#tic(); load(file = '../Massive_forests/cf_no_dins_2022-03-24.Rdata'); toc() # 

```


### B DINS analyses (smaller dataset, more predictors)
```{r}

# predicting severity

# random forest via party::cforest which accepts missing data and is slower
tic(); cf_dins <- party::cforest(
  damage_severity ~ . # severity, not binary
  , data = build_dins # notice build_dins here, no "no" in the object name.
  , controls =
    cforest_control(
        mtry = 5
      , ntree = n_tree # 10 ~0.52 seconds, 100 ~1.5s, 200 ~2.7s, 300 ~3.5s ... 500 ~7s
      , mincriterion = 0
      )
  ); toc()

# 3/20/2022 - Dexter, I wanted to see if switching to categorial outcomes added anything extra to our findings so
# I also included a DINS model with a binary outcome. I then compared the variable importance (top 20 variables, importance scores) for both DINS outcomes (binary, categorical) in section 4A. Because the outcomes are so similar I want to keep DINS as binary outcome - I think it's easier for descriptive figures and the reader, and it's keeping story simpler overall
#keeping this as binary - with dins
# subset
build_dins_binary <- build %>% 
  filter(build_DINS == 1) %>% 
  select(-build_DINS, -damage_severity)
  # dropping the DV for the smaller model
# random forest via party::cforest which accepts missing data and is slower
tic(); cf_dins_binary <- party::cforest(
  damage_cat ~ . # binary
  , data = build_dins_binary # notice build_dins_binary here, no "no" in the object name.
  , controls =
    cforest_control(
        mtry = 5
      , ntree = n_tree # 2000 is 80 sec - MHM
      , mincriterion = 0
      )
  ); toc()



# not sure if saving is needed..
# tic(); save(cf_dins
#              , file = paste0('../Massive_forests/cf_dins_', Sys.Date(), '.Rdata')); toc() # 3.5 seconds
# tic(); save(cf_dins_binary
#             , file = paste0('../Massive_forests/cf_dins_binary_', Sys.Date(), '.Rdata')); toc() # secs

# read back in
tic(); load(file = '../Massive_forests/cf_dins_2022-02-22.Rdata'); toc() # 1.5 seconds

#last date I ran this was 3/29/22 so will see that date below
tic(); load(file = '../Massive_forests/cf_dins_2022-03-29.Rdata'); toc() # 
tic(); load(file = '../Massive_forests/cf_dins_binary_2022-03-29.Rdata'); toc() # 
```


## 3 overall model accuracy
```{r}
# under construction :-/ DHL todo
#"100_old_hold.Rmd" has good code to pull from
#also the ROC curves from "### v how did the model perform?" chunk in "Philly_Land_Cover_Change.Rmd"

# rest of chunk taken from "Philly_Land_Cover_Change.Rmd"
# NEEDS TO BE EDITED

### v how did the model perform?

# check_collinearity(dat_multi_mod)
# check_collinearity(dat_multi_mod_idw)

# # it looks like we are over-predicting tree persistence
(tibble(obs = as.factor(dat_multi$delta), # observed
        pred= predict(dat_multi_mod)) %>% # predicted
    tabyl(obs, pred) -> ctab)


# nice worked example of multiclass ROC
# https://rdrr.io/cran/pROC/man/multiclass.html

# but this one is better
# https://github.com/WandeRum/multiROC
# get response categories as one hot encoded-tibble
# naming with multiROC::multi_roc is really finicky
# column names cannot have dots and must end with 
# "_true" for the observed data and "_pred_xx" 
# for the predicted data, where "_xx" is a method (like 
# 'mn' for multinomial or 'rf' for random forest)
# (
model.matrix(~0 + dat_multi$delta) %>% # one hot encoding trick
  as_tibble() %>% #glimpse()
  mutate_all(., ~as.integer(.)) %>% 
  rename_all(., ~str_remove_all(., "dat_multi\\$delta")) %>%
  rename_all(., ~paste0(., '_true')) %>%
  bind_cols(., 
            predict(dat_multi_mod, type = 'prob') %>% # multinomial, container
              as_tibble() %>%
              rename_all(., ~paste0(., '_pred_MNc')),
            # predict(dat_multi_mod_idw, type = 'prob') %>% # multinomial, IDW
            #   as_tibble() %>%
            #   rename_all(., ~paste0(., '_pred_MNi')),
            predict(dat_multi_mod_rf, type = 'prob') %>% # random forest, container
              as_tibble() %>% 
              rename_all(., ~paste0(., '_pred_RFc'))
            # predict(dat_multi_mod_rf_idw, type = 'prob') %>% # random forest, IDW
            #   as_tibble() %>% 
            #   rename_all(., ~paste0(., '_pred_RFi'))
  ) %>% 
  rename_all(., ~str_replace_all(., '\\.', '_')) %>%  
  data.frame() -> class_test

model.matrix(~0 + dat_multi_idw$delta) %>% # one hot encoding trick
  as_tibble() %>% #glimpse()
  mutate_all(., ~as.integer(.)) %>% 
  rename_all(., ~str_remove_all(., "dat_multi_idw\\$delta")) %>%
  rename_all(., ~paste0(., '_true')) %>%
  bind_cols(., 
            # predict(dat_multi_mod, type = 'prob') %>% # multinomial, container
            #   as_tibble() %>%
            #   rename_all(., ~paste0(., '_pred_MNc')),
            predict(dat_multi_mod_idw, type = 'prob') %>% # multinomial, IDW
              as_tibble() %>%
              rename_all(., ~paste0(., '_pred_MNi')),
            # predict(dat_multi_mod_rf, type = 'prob') %>% # random forest, container
            #   as_tibble() %>% 
            #   rename_all(., ~paste0(., '_pred_RFc')),
            predict(dat_multi_mod_rf_idw, type = 'prob') %>% # random forest, IDW
              as_tibble() %>%
              rename_all(., ~paste0(., '_pred_RFi'))
  ) %>% 
  rename_all(., ~str_replace_all(., '\\.', '_')) %>%  
  data.frame() -> class_test_idw  

# )


compare_performance(dat_multi_mod, dat_multi_mod_idw
                    , rank = TRUE
)

# model_performance(dat_multi_mod)
# model_performance(dat_multi_mod_idw)

dat_multi_mod_rf; dat_multi_mod_rf_idw

class_test %>% names
class_test %>% glimpse

class_test_idw %>% names
class_test_idw %>% glimpse


roc_res <- multiROC::multi_roc(class_test, force_diag=TRUE)
plot_roc_df <- plot_roc_data(roc_res)

roc_res_idw <- multiROC::multi_roc(class_test_idw, force_diag=TRUE)
plot_roc_df_idw <- plot_roc_data(roc_res_idw)




# roc_res$AUC
(auc <- tibble(variable             = roc_res$AUC$MNc %>% names()
               , multinomial_reg     =     roc_res$AUC$MNc %>% unlist()
               , multinomial_reg_idw = roc_res_idw$AUC$MNi %>% unlist()
               , random_forest       =     roc_res$AUC$RFc %>% unlist()
               , random_forest_idw   = roc_res_idw$AUC$RFi %>% unlist()
) |> 
    mutate_if(is.double, round, 2)
  # arrange(desc(multinomial_reg))
)

auc |> View()

auc |> 
  tidylog::select('Land Cover Transition' = variable, AUC = multinomial_reg) |> 
  mutate(  'Land Cover Transition' = str_replace_all(`Land Cover Transition`, '\\.', ' ')
           , 'Land Cover Transition' = str_to_title(`Land Cover Transition`)) |> 
  write_csv('outputs/AUC.csv')

# round (and transpose) kind of weird that this works
auc %>% dplyr::select(-variable) %>% map(., ~round(., 2)) |> bind_rows(.id = 'var') |> View()
# roc_res$AUC$MNc %>% flatten() |> bind_rows() |> str()


# AUC contains a list of AUC for each group of different classifiers. Micro-average ROC-AUC was calculated by stacking all groups together, thus converting the multi-class classification into binary classification. Macro-average ROC-AUC was calculated by averaging all groups results (one vs rest) and linear interpolation was used between points of ROC.

# (
#   roc_all <- plot_roc_df %>% 
#     bind_rows(plot_roc_df_idw) |> 
#     ggplot(aes(x = 1-Specificity, y = Sensitivity)) +
#     geom_path(aes(color = Group, linetype=Method), size = 1) + # , alpha = .5) +
#     geom_segment(aes(x = 0, y = 0, xend = 1, yend = 1), 
#                           colour='grey', linetype = 'dotdash') +
#     scale_y_continuous(expand = c(0, 0)) +
#     scale_x_continuous(expand = c(0, 0)) +
#     theme_bw() + 
#     theme(plot.title = element_text(hjust = 0.5), 
#                    legend.justification=c(1, 0), legend.position=c(.95, .05),
#                    legend.title=element_blank(), 
#                    legend.background = element_rect(fill=NULL, size=0.5, 
#                                                              linetype="solid", colour ="black")) + 
#     coord_fixed() + 
#     # facet_wrap(~Method) + 
#     NULL
# )

# ggsave(paste0(getwd(), '/figs/roc_all_', Sys.Date(), '.png')
#        , width = 6.5, height = 6.5)



(
  roc_all_facet <- plot_roc_df %>%
    bind_rows(plot_roc_df_idw) |> # adds in the idw
    ggplot(aes(x = 1-Specificity, y = Sensitivity)) +
    geom_path(aes(color = Group, linetype=Method), size = 1) + # , alpha = .5) +
    geom_segment(aes(x = 0, y = 0, xend = 1, yend = 1),
                 colour='grey', linetype = 'dotdash') +
    scale_y_continuous(expand = c(0, 0)) +
    scale_x_continuous(expand = c(0, 0)) +
    theme_bw() +
    theme(plot.title = element_text(hjust = 0.5),
          # legend.justification=c(1, 0), legend.position=c(.95, .05),
          legend.title=element_blank(),
          legend.background = element_rect(fill=NULL, size=0.5,
                                           linetype="solid", colour ="black")) +
    coord_fixed() +
    facet_wrap(~Method) +
    NULL
)

# ggsave(paste0(getwd(), '/figs/roc_all_facet_', Sys.Date(), '.png')
#        , width = 6.5, height = 6.5)



# https://github.com/WandeRum/multiROC/blob/master/README.md
# https://stats.stackexchange.com/questions/2151/how-to-plot-roc-curves-in-multiclass-classification
# https://cran.r-project.org/web/packages/pROC/pROC.pdf
# https://bookdown.org/chua/ber642_advanced_regression/multinomial-logistic-regression.html#checking-assumptionl-multicollinearity


```

## 4 variable importance
```{r}
# # modified from first chunk here:
# # https://bgreenwell.github.io/pdp/articles/pdp.html#other-vignettes
# # Variable importance plot (compare to randomForest::varImpPlot(boston_rf))
# vip(boston_rf, bar = FALSE, horizontal = FALSE, size = 1.5)  # Figure 1

# variable importance plot with random forest; fast
vip_rf_no_dins <- vip(rf_no_dins
    # , bar = FALSE       # doesn't seem to work "help(vip)" showed that it changed to "geom"
    # , geom = 'point'    # try "col" instead
    , geom = 'col'      # try "point" instead
    , horizontal = TRUE # vignette example had this as FALSE (label easier rotated)
    # , size = 1.5        # plausibly pertains to point size
    # see "num_features"
    # "Integer specifying the number of variable importance scores to plot. Default is 10."
    ) + 
  theme_bw(16)          # added some flare (ok just bigger axis text and black/white theme)

# check the plot
vip_rf_no_dins



# if we want to use color/shapes or facets to indicate the variable TYPE
# like we did with the cforest object before, it looks like vip::vi can be used
# to calculate the variable importance scores, then we can re-purpose the old
# code to add the colors symbols, etc.. . 'help(vip)' has an example starting with 
# "# Better yet, store the variable importance scores and then plot
# # vi_scores <- vi(model, method = "firm")". That's how I learned to extract importance
# from randomForest::randomForest models

# # since the second one takes 10 minutes, this code is commented out. Instead of re-running, just read a *.csv backin
# # not sure about this scaling. When scale is FALSE the values are on totally different scales
# # `rank = TRUE`
# tic(); vi_cf_no_dins <- vi(cf_no_dins, scale = TRUE) %>% mutate(Model = 'cf_no_dins', n_tree = n_tree); toc() # 10 mins
# tic(); vi_cf_dins    <- vi(cf_dins,    scale = TRUE) %>% mutate(Model =    'cf_dins', n_tree = n_tree); toc() # 1 min
# tic(); vi_cf_dins_binary    <- vi(cf_dins_binary,    scale = TRUE) %>% mutate(Model =    'cf_dins_binary', n_tree = n_tree); toc() # 1 min
# 
# 
 # bind_rows(vi_cf_no_dins, vi_cf_dins,vi_cf_dins_binary) %>% # combines variable importance from each model
 #    write_csv(., paste0(getwd(), '/data/variable_importance', Sys.Date(), '.csv'))

 
 
# TODO join in aliases and type, repurpose directly below.
 #MHM -3/23/2022 - let's hold off on displaying scale in variable importance plots until we talk more w alex, I think theme is somewhat repeating this information and would like her take
(
  variable_importance <- read_csv(paste0(getwd(), '/data/variable_importance_2022-03-29.csv')) %>% 
    group_by(Model) %>% 
    mutate(rank = row_number(Model)) %>% 
    ungroup()
  )
 

# # get tab_lab
# left_join(
#       tab_labs %>% 
#         tidylog::select(`Variable name`, Alias, Type, Scale)) 

```
### A. compare DINS - class vs binary
```{r}
#compare variable importance outcomes for two ways of considering DINS outcomes - 
# DINS RF with binary outcome and categorial outcome
# 3/20/2022 - Dexter, I   compared the variable importance (top 20 variables, importance scores) for both DINS outcomes (binary, categorical) in section 4A. Because the outcomes are so similar I want to keep DINS as binary outcome - I think it's easier for descriptive figures and the reader, and it's keeping story simpler overall
#first, are same variables listed in top 20? #for most part, yes
variable_importance %>% 
  filter (rank <21)  %>% 
    filter (Model=='cf_dins' | Model== 'cf_dins_binary')  %>% 
      add_count(Variable) %>% 
        filter (n==1) 
#for the most part- there are two variables in top 2 for each model that are unique to that model
#cf_dins - categorial outcome includes  parcel_Paved_P & parcel_year_built but cf_dins_binary - binary outcome does not
#cf_dins_binary - binary outcome includes build_PATIOCOVER and build_p_building_2000 but cf_dins - categorial outcome does not
#second, are variable importances correlated?
#yes, highly correlated
variable_importance %>% 
 filter (Model=='cf_dins' | Model== 'cf_dins_binary') %>%
      dplyr::select(Variable, Importance, Model) %>% 
        pivot_wider(
          names_from = "Model",
          values_from = "Importance") ->Imp_wide
 
ggscatter(Imp_wide, x = "cf_dins", y = "cf_dins_binary", 
          add = "reg.line", conf.int = TRUE, 
          cor.coef = TRUE, cor.method = "pearson",
          xlab = "Imp_cf_bins - Category", ylab = "Imp_cf_dins - Binary")
  
```

## 5 partial dependence plots
### A base-style graphs (example only)
```{r eval=FALSE, include=FALSE}
# single base-graphics partial dependency plot, modeled after Figure 2 here:
# https://bgreenwell.github.io/pdp/articles/pdp.html#other-vignettes
# partialPlot(boston_rf, pred.data = boston, x.var = "lstat")  # Figure 2
randomForest::partialPlot(rf_no_dins
                          , pred.data = build_no_dins
                          , x.var = "build_near_dest"
                          ) # ugly!
```


### B ggplot graphs
#### i test on a single predictor
```{r}
# # building off of the third chunk from
# # https://bgreenwell.github.io/pdp/articles/pdp.html#other-vignettes
# # AND third chunk from this vignette
# # https://bgreenwell.github.io/pdp/articles/pdp-computation.html
# # Switch to ggplot2
# p2 <- partial(boston_rf, pred.var = "lstat", plot = TRUE,
#               plot.engine = "ggplot2")

# THIS IS A SINGLE VARIBLE TEST
pdp::partial(rf_no_dins                      # random forest model
             , pred.var = "build_near_dest"  # predictor variable
             , plot = TRUE                   # if FALSE returns prediction data
             , plot.engine = "ggplot2"       # plot with ggplot2 or not?
             , rug = TRUE                    # adds the univariate y-axis summary (aka "rug")
             , progress = "text"             # prints progress, sets users expectations
             , quantiles = TRUE              # see help(pdp)
             , probs = 0:20/20               # probabilities, "0:20/20" gives 5% increments
             , ice = TRUE                    # Individual Conditional Expectations
             , center = TRUE                 # for graphical layout, pertains to ice
             , alpha = .1                    # line transparency
             ) + 
  theme_bw(16) +                             # ggplot adjustment for black/white & large text size 
  # scale_y_continuous(expand = c(0, 0)) +
  NULL -> test_pdp                           # makes the plot

test_pdp # accesses plot

variable <- "build_near_dest"
ggsave(file = paste0(getwd(), '/figures/cf_dins/', variable, '_', Sys.Date(), '.png')
       , width = 6.5*2
       , height = 6.5*1.5
       , dpi = 100)
```

#### ii non-DINS RF
```{r}
# TODO
```

#### iii non-DINS CF
```{r}
# TODO Miranda, we can look at all plots, but maybe we just want to look at the top 10 or 20 top predictors
#2/23/22 Dexter I think top 10 seems fine, the Price MS only shows several in final MS.
## CF non-DINS
# loop through predictors to get PDP
# first get the iterator, here the variable names

# using ALL predictors
(pdp_cf_no_dins_vars <- variable_importance %>% filter(Model == 'cf_no_dins') %>%                 pull(Variable))

# using top n predictors
top_n <- 10          # set n (can revise to 20 or any other value greater than zero and less than 71)
(pdp_cf_no_dins_vars <- variable_importance %>% filter(Model == 'cf_no_dins') %>% slice(1:top_n) %>% pull(Variable)) 


resolution <- 100                       # dots per inch (dpi) for figure output (lower values keep files size down,
                                        # can increase for publication)
tic()                                   # clock in for whole loop
for(variable in pdp_cf_no_dins_vars){   # start loop, iterating over pdp_cf_no_dins_vars
  print(variable)                       # just to track the progress
  tic()                                 # clock in within the loop
  pdp::partial(cf_dins                  # the model
             , pred.var = variable  
             , plot = TRUE              
             , plot.engine = "ggplot2"  
             , rug = TRUE               
             , progress = "text"        
             , quantiles = TRUE         
             , probs = 0:20/20          
             , ice = TRUE               
             , center = TRUE            
             , alpha = .1               
             ) + 
    theme_bw(16) +                       # ggplot adjustment for black/white & large text size 
    # scale_y_continuous(expand = c(0, 0)) +
    labs(title = variable) +
    NULL
  
  # save out
  ggsave(file = paste0(getwd(), '/figures/cf_no_dins/', variable, '_', Sys.Date(), '.png')
         , width = 6.5*2
         , height = 6.5*1.5
         , dpi = resolution)
  toc()    # clock out within loop
  }; toc() # clock out whole loop



```

#### iv DINS CF
```{r}
# TODO Miranda, we can look at all plots, but maybe we just want to look at the top 10 or 20 top predictors
#2/23/22 Dexter I think top 10 seems fine, the Price MS only shows several in final MS.

## CF DINS #these are categorical outcomes, 4/4/2022, let's replace with next section with binary outcomes
# loop through predictors to get PDP
# first get the iterator, here the variable names

# using ALL predictors
(pdp_cf_dins_vars <- variable_importance %>% filter(Model == 'cf_dins') %>%                 pull(Variable))

# using top n predictors
top_n <- 10          # set n (can revise to 20 or any other value greater than zero and less than 71)
(pdp_cf_dins_vars <- variable_importance %>% filter(Model == 'cf_dins') %>% slice(1:top_n) %>% pull(Variable)) 


resolution <- 100                       # dots per inch (dpi) for figure output (lower values keep files size down,
                                        # can increase for publication)
tic()                                   # clock in for whole loop
for(variable in pdp_cf_dins_vars){      # start loop
  print(variable)                       # just to track the progress
  tic()                                 # clock in within the loop
  pdp::partial(cf_dins                  # the model
             , pred.var = variable  
             , plot = TRUE              
             , plot.engine = "ggplot2"  
             , rug = TRUE               
             , progress = "text"        
             , quantiles = TRUE         
             , probs = 0:20/20          
             , ice = TRUE               
             , center = TRUE            
             , alpha = .1               
             ) + 
    theme_bw(16) +                       # ggplot adjustment for black/white & large text size 
    # scale_y_continuous(expand = c(0, 0)) +
    labs(title = variable) +
    NULL
  
  
  # save out
  ggsave(file = paste0(getwd(), '/figures/cf_dins/', variable, '_', Sys.Date(), '.png')
         , width = 6.5*2
         , height = 6.5*1.5
         , dpi = resolution)
  toc()    # clock out within loop
  }; toc() # clock out whole loop



```


#### v DINS CF - binary outcome
```{r}
# TODO Miranda, we can look at all plots, but maybe we just want to look at the top 10 or 20 top predictors
#2/23/22 Dexter I think top 10 seems fine, the Price MS only shows several in final MS.

## CF DINS BINARY cf_dins_binary
# loop through predictors to get PDP
# first get the iterator, here the variable names

# using ALL predictors
(pdp_cf_dins_bin_vars <- variable_importance %>% filter(Model == 'cf_dins_binary') %>%                 pull(Variable))

# using top n predictors
top_n <- 10          # set n (can revise to 20 or any other value greater than zero and less than 71)
(pdp_cf_dins_bin_vars <- variable_importance %>% filter(Model == 'cf_dins_binary') %>% slice(1:top_n) %>% pull(Variable)) 


resolution <- 100                       # dots per inch (dpi) for figure output (lower values keep files size down,
                                        # can increase for publication)
tic()                                   # clock in for whole loop
for(variable in pdp_cf_dins_bin_vars){      # start loop
  print(variable)                       # just to track the progress
  tic()                                 # clock in within the loop
  pdp::partial(cf_dins_binary    # the model
             , pred.var = variable  
             , plot = TRUE              
             , plot.engine = "ggplot2"  
             , rug = TRUE               
             , progress = "text"        
             , quantiles = TRUE         
             , probs = 0:20/20          
             , ice = TRUE               
             , center = TRUE            
             , alpha = .1               
             ) + 
    theme_bw(16) +                       # ggplot adjustment for black/white & large text size 
    # scale_y_continuous(expand = c(0, 0)) +
    labs(title = variable) +
    NULL
  
  # save out
  ggsave(file = paste0(getwd(), '/figures/cf_dins_binary/', variable, '_', Sys.Date(), '.png')
         , width = 6.5*2
         , height = 6.5*1.5
         , dpi = resolution)
  toc()    # clock out within loop
  }; toc() # clock out whole loop


```


```{r, citations}
lapply(packages, citation)
```


Last knit on `r format(Sys.time())`


```{r}
system.time(save.image(file = paste0('saved_sessions/wui_r_models_', gsub('[[:punct:]]', '-', Sys.time()), '.RData')))
```

