---
title: "04_fit_models"
author: "Dexter H. Locke, PhD"
date: "`r format(Sys.time())`"
output: html_document
editor_options: 
  chunk_output_type: console
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

This assumes "02_Woosley_combine_data.Rmd" was run



## 0 set up: load libraries, custom functions, set defaults
```{r}
# load libraries
# packages we'll be using
packs <- c('tidyverse'        # a must have!
           , 'tidylog'        # makes things very verbose for 2x checking 
           , 'magrittr'       # all of the pipes
           , 'janitor'        # cleans things up
           , 'sf'             # simple features
           , 'mapview'        # quick webmaps for zoom/pan viz
           #'tidycensus',     # access to Census data in a tidy way
           #'party',          # random forests
           , 'modEvA'         # contains D-squared function
           , 'randomForest', 'rfUtilities', 'verification'
           , 'tictoc'         # times things
           , 'beepr'          # makes noises
           , 'broom'          # tidy up regression models
           , 'performance'    # nice regression diagnostics
           , 'psych'          # describe is very useful for descriptive statistics
           , 'sjPlot')        # useful plotting and regression support

# check for all of the libraries
if (length(setdiff(packs, rownames(installed.packages()))) > 0) {
  install.packages(setdiff(packs, rownames(installed.packages())))  
}

lapply(packs, library, character.only = TRUE)

set.seed(19870630)
```



## 1 read in the data
```{r}
build <- read_csv(paste0(getwd(), '/output_data/building_2021-04-26.csv')) %>% glimpse
```



## 2 fit models
### A univariate-logistic regression to select from potentially redundant predictors
```{r}
tic(); uni_log_mods <- build %>%
  # nest(data=everything()) %>%
  nest()%>%
  mutate(
    # ~landscape position
    log_build_Mean_elev_30 = map(., ~glm(damage_binary ~ build_Mean_elev_30m, family = binomial, data = data.frame(.))),
    log_build_Mean_elev_100= map(., ~glm(damage_binary ~ build_Mean_elev_100m,family = binomial, data = data.frame(.))),
    
    log_build_Mean_slope_30 = map(., ~glm(damage_binary ~ build_Mean_slope_30m_DEM, family = binomial, data = data.frame(.))),
    log_build_Mean_slope_100= map(., ~glm(damage_binary ~ build_Mean_slope_100m_DEM,family = binomial, data = data.frame(.))),
    
    log_build_Mean_aspect_30 = map(., ~glm(damage_binary ~ build_Mean_aspect_30m_DEM, family= binomial, data = data.frame(.))),
    log_build_Mean_aspect_100= map(., ~glm(damage_binary ~ build_Mean_aspect_100m_DEM,family= binomial, data = data.frame(.))),
    
    # land cover
    log_build_p_tree_10 = map(., ~glm(damage_binary ~ build_p_tree_10, family = binomial, data = data.frame(.))),
    log_build_p_tree_100= map(., ~glm(damage_binary ~ build_p_tree_100,family = binomial, data = data.frame(.))),
    log_build_p_tree_200= map(., ~glm(damage_binary ~ build_p_tree_200,family = binomial, data = data.frame(.))),
    log_build_p_tree_300= map(., ~glm(damage_binary ~ build_p_tree_300,family = binomial, data = data.frame(.))),
  
    log_build_p_grass_10 = map(., ~glm(damage_binary ~ build_p_grass_10, family = binomial, data = data.frame(.))),
    log_build_p_grass_100= map(., ~glm(damage_binary ~ build_p_grass_100,family = binomial, data = data.frame(.))),
    log_build_p_grass_200= map(., ~glm(damage_binary ~ build_p_grass_200,family = binomial, data = data.frame(.))),
    log_build_p_grass_300= map(., ~glm(damage_binary ~ build_p_grass_300,family = binomial, data = data.frame(.))),
    
    log_build_p_soil_10 = map(., ~glm(damage_binary ~ build_p_soil_10, family = binomial, data = data.frame(.))),
    log_build_p_soil_100= map(., ~glm(damage_binary ~ build_p_soil_100,family = binomial, data = data.frame(.))),
    log_build_p_soil_200= map(., ~glm(damage_binary ~ build_p_soil_200,family = binomial, data = data.frame(.))),
    log_build_p_soil_300= map(., ~glm(damage_binary ~ build_p_soil_300,family = binomial, data = data.frame(.))),
    
    log_build_p_water_10 = map(., ~glm(damage_binary ~ build_p_water_10, family = binomial, data = data.frame(.))),
    log_build_p_water_100= map(., ~glm(damage_binary ~ build_p_water_100,family = binomial, data = data.frame(.))),
    log_build_p_water_200= map(., ~glm(damage_binary ~ build_p_water_200,family = binomial, data = data.frame(.))),
    log_build_p_water_300= map(., ~glm(damage_binary ~ build_p_water_300,family = binomial, data = data.frame(.))),
    
    log_build_p_buliding_10 = map(., ~glm(damage_binary ~ build_p_building_10, family = binomial, data = data.frame(.))),
    log_build_p_buliding_100= map(., ~glm(damage_binary ~ build_p_building_100,family = binomial, data = data.frame(.))),
    log_build_p_buliding_200= map(., ~glm(damage_binary ~ build_p_building_200,family = binomial, data = data.frame(.))),
    log_build_p_buliding_300= map(., ~glm(damage_binary ~ build_p_building_300,family = binomial, data = data.frame(.))),
    
    log_build_p_road_10 = map(., ~glm(damage_binary ~ build_p_road_10, family = binomial, data = data.frame(.))),
    log_build_p_road_100= map(., ~glm(damage_binary ~ build_p_road_100,family = binomial, data = data.frame(.))),
    log_build_p_road_200= map(., ~glm(damage_binary ~ build_p_road_200,family = binomial, data = data.frame(.))),
    log_build_p_road_300= map(., ~glm(damage_binary ~ build_p_road_300,family = binomial, data = data.frame(.))),
    
    log_build_p_otherpaved_10 = map(., ~glm(damage_binary ~ build_p_otherpaved_10, family = binomial, data = data.frame(.))),
    log_build_p_otherpaved_100= map(., ~glm(damage_binary ~ build_p_otherpaved_100,family = binomial, data = data.frame(.))),
    log_build_p_otherpaved_200= map(., ~glm(damage_binary ~ build_p_otherpaved_200,family = binomial, data = data.frame(.))),
    log_build_p_otherpaved_300= map(., ~glm(damage_binary ~ build_p_otherpaved_300,family = binomial, data = data.frame(.))),
    
    log_build_p_shrub_10 = map(., ~glm(damage_binary ~ build_p_shrub_10, family = binomial, data = data.frame(.))),
    log_build_p_shrub_100= map(., ~glm(damage_binary ~ build_p_shrub_100,family = binomial, data = data.frame(.))),
    log_build_p_shrub_200= map(., ~glm(damage_binary ~ build_p_shrub_200,family = binomial, data = data.frame(.))),
    log_build_p_shrub_300= map(., ~glm(damage_binary ~ build_p_shrub_300,family = binomial, data = data.frame(.))),

    log_build_dist_buliddistdhl_0= map(., ~glm(damage_binary ~ build_dist_to_build_ft,family = binomial, data = data.frame(.))),
    log_build_dist_builddistjod_0= map(., ~glm(damage_binary ~ build_dist_to_build_ft_jod,family = binomial, data = data.frame(.)))
    ) %>%
  dplyr::select(-data) %>% 
  rowid_to_column() %>% 
  pivot_longer(-rowid, names_to = 'model_name', values_to = 'model') %>% 
  dplyr::select(-rowid) %>% 
  separate(col = model_name, into = c('trash_1', 'trash_2', 'trash_3', 'var', 'dist'), remove = FALSE, convert = TRUE) %>% 
  dplyr::select(-starts_with('trash_')) %>% 
  mutate(#smry = map(model, summary),
         dsqr = map(model, Dsquared),
         AIC = map(model, AIC),
         r2 = map(model, performance::r2),
         rmse = map(model, performance::rmse)); toc()


# extract goodness of fit measures
(
  gof <- uni_log_mods %>% 
    dplyr::select(-model_name, -model) %>% 
    #dplyr::select(var, dist, AIC, dsqr, r2) %>% 
    #unnest(dsqr, AIC, r2) %>% unnest(r2)
    unnest(everything()) %>% 
    unnest(everything())
)


# which distance provides the best fit per variable?
# FIXME get building dist 2 build to be less bad
(
  best_dist <- gof %>% 
    group_by(var) %>% 
    summarise(max_d = dist[which.max(dsqr)],
              max_r = dist[which.max(r2)],
              min_AIC=dist[which.min(AIC)],
              min_rmse=dist[which.min(rmse)]) 
)

```



### B data prep
```{r}

# maximimal set of predictors after winnowing down among potentially redundant vars
log_build_reg_data <- build %>%
  tidylog::select(damage_binary
                  , build_Mean_elev_30m
                  ,  build_Mean_aspect_100m_DEM
                  ,  build_Mean_slope_100m_DEM
                  ,  build_Mean_aspect_100m_DEM 
                  ,  build_Mean_distroad
                  ,  build_Mean_builddens 
                  ,  build_Mean_distall_road 
                  ,  build_min_dist_tree
                  ,  build_min_dist_shrub 
                  
                  , build_has_tree_overhang
                  , build_overhang_ht
                  , build_perc_overhang
                  
                  , build_dist_angle
                  , build_dist_to_build_ft_jod
                  
                  , build_p_tree_100 
                  , build_p_grass_300 
                  , build_p_soil_200 
                  , build_p_water_100 
                  , build_p_building_300 
                  , build_p_road_300
                  , build_p_otherpaved_300
                  , build_p_shrub_300
                    
                  , build_area
                  
                  , parcel_UseType
                    # parcel_UseDescription, # colinear with above?
                    # parcel_Roll_ImpValue, # colinear with below?
                  , parcel_Roll_HomeOwnersExemp
                  , parcel_Can_P
                  , parcel_Grass_P
                  , parcel_Soil_P 
                  , parcel_Water_P
                  , parcel_Build_P
                  , parcel_Road_P
                  , parcel_Paved_P
                  , parcel_Shrub_P
                  , parcel_Perv_P
                  , parcel_Imperv_P
                  , parcel_year_built
                  , parcel_area) %>% 
  mutate_if(is.character, as.factor) %>% # random forest doesn't like characters
  drop_na()


```



### C fit: null, maximal, stepwise logistic regression
```{r}
  
# Null model
log_null<- glm(damage_binary ~ 1, family = binomial, data = log_build_reg_data)

# maximal model 
log_maximal<- glm(damage_binary ~ ., family = binomial, data = log_build_reg_data)

# step-wise
tic();log_step <- log_maximal %>% MASS::stepAIC(trace = FALSE); toc() # ~1.5 mins

# store chosen terms from stepwise regression
(log_step %>% 
    tidy() %>% 
    filter(term != '(Intercept)') %>% 
    pull(term) -> vars_in_log_step)

```



### D examine fits
```{r}
performance::compare_performance(log_null, log_maximal, log_step, rank = TRUE) # chooses stepwise

modEvA::Dsquared(log_maximal)
modEvA::Dsquared(log_step)

# find out limts of odds ratios, to inform graph settings
round(sort(exp(coef(log_step))), 2) %T>% print() %>% range() # 0 to ~1.6

# tab_model(log_maximal, show.std = FALSE)
plot_model(log_step, type = "std", vline.color = 'black', sort.est = TRUE) + 
  ylim(0, 1.65) + theme_bw()

# ggsave(file = paste0(getwd(), '/figures/stepwise_logistic_reg_coefs_',
#                      gsub('[[:punct:]]', '_', Sys.time()), '.png'))

tab_model(log_step
          # , file = paste0(getwd(), '/output_data/stepwise_logistic_reg_coefs_',
          #               gsub('[[:punct:]]', '_', Sys.time()), '.html')
          ); warnings()


# is that regression model any good?
performance::check_model(log_step) # saving isn't so easy

```


http://www.sthda.com/english/articles/36-classification-methods-essentials/151-logistic-regression-essentials-in-r/

### E assess missingness and stepwise model
```{r}

# of the variables that made it through the stepwise procedure, which had missing vals?
(
  build %>%
    map(., ~sum(is.na(.))) %>%
    map_dfc(., ~.x) %>% 
    t() %>%
    data.frame() %>%
    rownames_to_column(var = 'var') %>% 
    rename(num_missing = '.') %>% 
    arrange(desc(num_missing)) %>%
    filter(var %in% vars_in_log_step) %>% 
    filter(num_missing > 0) -> missing_data_vars
)

# These are the ones to think about
#                  var num_missing
#1       parcel_year_built        1008 #this is important in RF; impute about median.
#2     build_Mean_distroad          88 #this is important in RF; impute the median. #TODO - Dexter I think we should just drop these, seems weirder to impute a median here.
#3       build_overhang_ht          47 #not important in maximal RF model
#4     build_perc_overhang          47 #not important in maximal RF model
#5 build_Mean_distall_road          12 #not important in maximal RF model 
```



### F impute missing values AND drop the variables previously determined to be low-importance
```{r}
build %>% 
  mutate_all(~ifelse(is.na(.), median(., na.rm = TRUE), .)) %>% 
  tidylog::select(-build_overhang_ht, -build_perc_overhang, -build_Mean_distall_road)
  
```




### C fit Jeffrey Evan's worked example: randomForest
```{r}
# modeled after and adapted from this page
# from https://evansmurphy.wixsite.com/evansspatial/random-forest-sdm

# abbridged from J Evan's second chunk (chunk 1 accesses data)
# library(randomForest)
# library(rfUtilities)

b <- 1001                                     # Number of Bootstrap replicates
# snip
# chunks 3 and 4 import raster dadta and extract variables from raster to absence/present point locations
# snip

log_build_reg_data

log_build_reg_data %>% tabyl(damage_binary)

# remove build_has_tree_overhang?
log_build_reg_data %<>% mutate(damage_binary = as.factor(damage_binary)) %>% data.frame()# PAY ATTENTION HERE
log_build_reg_data %>% glimpse

rf_collinearity_test <- log_build_reg_data %>% 
  tidylog::select(where(is.double))

(cl <- multi.collinear(rf_collinearity_test[,2:ncol(rf_collinearity_test)], p = 0.05))

# tell us what's got to go!
for(l in cl) {
  cl.test <- rf_collinearity_test[,-which(names(rf_collinearity_test)==l)]
  print(paste("Remove variable", l, sep=": "))
  multi.collinear(cl.test, p=0.05)
}


log_build_reg_data_cl <- log_build_reg_data[,-which(names(log_build_reg_data) %in% cl )] 
# %>% # should not be needed any more
#   data.frame() %>%
#   mutate_if(is.character, as.factor)

# Chunk 8 makes depenent variable categorical (done). BUT it also checks for frequency of presense

# "We observe that the sample balance of presence locations is 33% thus, meeting the 1/3 rule for sample balance."

log_build_reg_data_cl %>% tabyl(damage_binary)
print('12% - is this a problem?!?!')

# model selection
# ( rf.model <- rf.modelSel(x=sdata@data[,3:ncol(sdata@data)], y=sdata@data[,"Present"], imp.scale="mir", ntree=b) )

tic() # clock in
(rf.model <- rf.modelSel(x=log_build_reg_data_cl[,2:ncol(log_build_reg_data_cl)],
                         y=log_build_reg_data_cl[,"damage_binary"],
                         imp.scale="mir", ntree=b))
toc() # clock out, about 3 minutes
beepr::beep()


sel.vars <- rf.model$selvars

# OR, pick a model?
#sel.vars <- rf.model$parameters[[4]] # DECISION POINT

# run a model
tic() # clock in
(rf.fit <- randomForest(y=log_build_reg_data_cl[,"damage_binary"],
                        x=log_build_reg_data_cl[,sel.vars],
                        ntree=b,
                        importance=TRUE, norm.votes=TRUE, proximity=TRUE) )
toc() # clock out, about 3 minutes
beepr::beep()

  
  # run a model with ALL PREDICTORS
  tic() # clock in
  ( rf.fit_all <- randomForest(y=log_build_reg_data_cl[,"damage_binary"],
                               x=log_build_reg_data_cl[,2:ncol(log_build_reg_data_cl)],
                           ntree=b,
                           importance=TRUE, norm.votes=TRUE, proximity=TRUE) )
  toc() # clock out, about 4 minutes
  beepr::beep()


tic() # clock in
( imbal <- randomForestSRC::imbalanced(damage_binary~., data=log_build_reg_data_cl) )
toc() # clock out, about 3 minutes
beepr::beep()

# skipping chunks 13 and 14: predicted raster map.. doesn't really apply here.

# model fit!
rf.pred <- predict(rf.fit, log_build_reg_data_cl[,sel.vars], type="response")

rf.prob <- as.data.frame(predict(rf.fit, log_build_reg_data_cl[,sel.vars], type="prob"))

obs.pred <- data.frame(cbind(Observed=as.numeric(as.character(log_build_reg_data_cl[,"damage_binary"])),
                             PRED=as.numeric(as.character(rf.pred)), Prob1=rf.prob[,2],
                             Prob0=rf.prob[,1]) )

op <- (obs.pred$Observed == obs.pred$PRED)



( pcc <- (length(op[op == "TRUE"]) / length(op))*100 )



# library(verification)

roc.plot(obs.pred[,"Observed"], obs.pred[,"Prob1"])


# model validation
tic() # clock in
( rf.perm <- rf.significance(rf.fit, log_build_reg_data_cl[,sel.vars], nperm = 99, ntree = 1001) )
toc() # clock out, 59 minutes with 99 permutations
beepr::beep()

# saveRDS(rf.perm, file = paste0("saved_sessions/rf.perm",
#                                gsub('[[:punct:]]', '-', Sys.time()), '.rds'))

# cross validation
tic() # clock in
( rf.cv <- rf.crossValidation(rf.fit, log_build_reg_data_cl[,sel.vars], p=0.10, n=99, ntree=1001) )
toc() # clock out, about 143 minutes (2.38 hours) with 999 permutations
beepr::beep()

# so this model isn't very good

saveRDS(rf.perm, file = paste0("saved_sessions/rf.perm",
                               gsub('[[:punct:]]', '-', Sys.time()), '.rds'))


# so the number of trees is probably over kill at 1000? 75 sufficient?
plot(rf.fit, main="Bootstrap Error Convergence")


# variable importance
p <- as.matrix(rf.fit$importance[,3])   
ord <- rev(order(p[,1], decreasing=TRUE)[1:dim(p)[1]]) 


png(file = paste0(getwd(), '/figures/var_imp_', gsub('[[:punct:]]', '_', Sys.time()), '.png'))
dotchart(p[ord,1], main="Scaled Variable Importance", pch=19)  
dev.off()


p <- as.matrix(rf.fit_all$importance[,3])   
ord <- rev(order(p[,1], decreasing=TRUE)[1:dim(p)[1]]) 


png(file = paste0(getwd(), '/figures/var_imp_all_', gsub('[[:punct:]]', '_', Sys.time()), '.png'))
dotchart(p[ord,1], main="Scaled Variable Importance", pch=19)  
dev.off()

(all_vars <- names(log_build_reg_data_cl))

#TODO: PDP don't like categorical predictors.. 
# all_vars <-  all_vars[c(1:9, 11:35)]


# `%nin%` <- Negate(`%in%`) # custom function
# 
# # drop the binary
# (all_vars <- all_vars[all_vars %nin% 'build_has_tree_overhang'])

tic()
par(mfrow=c(1,1))
for(i in all_vars[2:length(all_vars)]) {
  print(i)
  png(file = paste0(getwd(), '/figures/', i, '_pdp',
                  gsub('[[:punct:]]', '_', Sys.time()), '.png'))
  rf.partial.prob(rf.fit_all, log_build_reg_data_cl[,all_vars], i, "1", smooth="spline", raw.line=FALSE)
  dev.off()
}; toc()  



# par(mfrow=c(2,2))
# for(i in sel.vars[1:4]) {
#   rf.partial.prob(rf.fit, build_rforest_cat[,sel.vars], i, "1", smooth="spline", raw.line=FALSE)
# }  

tic()
for(i in sel.vars) {
png(file = paste0(getwd(), '/figures/', i, '_pdp_',
                  gsub('[[:punct:]]', '_', Sys.time()), '.png'))
rf.partial.prob(rf.fit,
                log_build_reg_data_cl[,sel.vars], i, "1", smooth="spline", raw.line=FALSE)
dev.off()
  }; toc() 
beep()
```







#### i random forest readings

Thanks Sebastian Martinuzzi

https://evansmurphy.wixsite.com/evansspatial/random-forest-sdm

https://cran.r-project.org/web/packages/rfUtilities/index.html
(Utilities for Random Forest model selection, class balance correction, significance test, cross validation and partial dependency plots.)

https://www.rdocumentation.org/packages/randomForestSRC/versions/2.9.3/topics/imbalanced.rfsrc
(Imbalanced Two Class Problems)

Old, but important
https://statistics.berkeley.edu/sites/default/files/tech-reports/666.pdf

A review paper on "A survey on addressing high-class imbalance in big data"
https://link.springer.com/article/10.1186/s40537-018-0151-6

https://towardsdatascience.com/random-forest-in-r-f66adf80ec9

http://www.sthda.com/english/articles/36-classification-methods-essentials/150-stepwise-logistic-regression-essentials-in-r/#:~:text=The%20stepwise%20logistic%20regression%20can,ref(stepwise%2Dregression))

https://compstat-lmu.github.io/iml_methods_limitations/pdp.html

https://www.blopig.com/blog/2017/04/a-very-basic-introduction-to-random-forests-using-r/





### OLD A a little prep
```{r}
# TODO handle the NA's better than dropping 11%
# a) manual fill, b) impute - see if it matters in the model
# see if year built is even a good predictor

# minor prep for binary 
build_bin <- build %>% 
  st_drop_geometry() %>% 
  select(damage_binary, build_Mean_elev_30m : build_Mean_distall_road,
         build_NEAR_DIST, build_NEAR_ANGLE,
         build_dist_to_build_ft : build_p_shrub_300,
         parcel_UseType: parcel_Imperv_P,
         parcel_year_built) %>% 
  filter(parcel_year_built > 0) %>% 
  drop_na() %>% 
  data.frame #%>% glimpse

# check for NA
map(build_bin, ~sum(is.na(.))) # check for NA's

build_bin %>% filter(parcel_year_built == 0) %>% dim()

```




```{r, citations}
lapply(packages, citation)
```


Last knit on `r format(Sys.time())`


```{r}
system.time(save.image(file = paste0('saved_sessions/wui_r_models_', gsub('[[:punct:]]', '-', Sys.time()), '.RData')))
```

