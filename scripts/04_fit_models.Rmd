---
title: "04_fit_models"
author: "Dexter H. Locke, PhD"
date: "`r format(Sys.time())`"
output: html_document
editor_options: 
  chunk_output_type: console
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

This assumes "02_Woosley_combine_data.Rmd" was run



## 0 set up: load libraries, custom functions, set defaults
```{r}
# load libraries
# packages we'll be using
packs <- c('tidyverse'        # a must have!
           , 'tidylog'        # makes things very verbose for 2x checking 
           , 'magrittr'       # all of the pipes
           , 'janitor'        # cleans things up
           , 'sf'             # simple features
           , 'mapview'        # quick webmaps for zoom/pan viz
           #'tidycensus',     # access to Census data in a tidy way
           #'party',          # random forests
           , 'modEvA'         # contains D-squared function
           , 'randomForest', 'rfUtilities', 'verification'
           , 'tictoc'         # times things
           , 'beepr'          # makes noises
           , 'psych'          # describe is very useful for descriptive statistics
           , 'sjPlot')        # useful plotting and regression support

# check for all of the libraries
if (length(setdiff(packs, rownames(installed.packages()))) > 0) {
  install.packages(setdiff(packs, rownames(installed.packages())))  
}

lapply(packs, library, character.only = TRUE)
```



## 1 read in the data
```{r}
build <- read_csv(paste0(getwd(), '/output_data/building_2021-03-24.csv')) %>% 
  filter(build_min_dist_tree < Inf)
```





## 2 fit models
### A univariate-logistic regression
```{r}


tic(); uni_log_mods <- build %>%
  nest() %>%
  mutate(
    # ~landscape position
    log_build_Mean_elev_30 = map(., ~glm(damage_binary ~ build_Mean_elev_30m, family = binomial, data = data.frame(.))),
    log_build_Mean_elev_100= map(., ~glm(damage_binary ~ build_Mean_elev_100m,family = binomial, data = data.frame(.))),
    
    log_build_Mean_slope_30 = map(., ~glm(damage_binary ~ build_Mean_slope_30m_DEM, family = binomial, data = data.frame(.))),
    log_build_Mean_slope_100= map(., ~glm(damage_binary ~ build_Mean_slope_100m_DEM,family = binomial, data = data.frame(.))),
    
    log_build_Mean_aspect_30 = map(., ~glm(damage_binary ~ build_Mean_aspect_30m_DEM, family= binomial, data = data.frame(.))),
    log_build_Mean_aspect_100= map(., ~glm(damage_binary ~ build_Mean_aspect_100m_DEM,family= binomial, data = data.frame(.))),
    
    # land cover
    log_build_p_tree_10 = map(., ~glm(damage_binary ~ build_p_tree_10, family = binomial, data = data.frame(.))),
    log_build_p_tree_100= map(., ~glm(damage_binary ~ build_p_tree_100,family = binomial, data = data.frame(.))),
    log_build_p_tree_200= map(., ~glm(damage_binary ~ build_p_tree_200,family = binomial, data = data.frame(.))),
    log_build_p_tree_300= map(., ~glm(damage_binary ~ build_p_tree_300,family = binomial, data = data.frame(.))),
  
    log_build_p_grass_10 = map(., ~glm(damage_binary ~ build_p_grass_10, family = binomial, data = data.frame(.))),
    log_build_p_grass_100= map(., ~glm(damage_binary ~ build_p_grass_100,family = binomial, data = data.frame(.))),
    log_build_p_grass_200= map(., ~glm(damage_binary ~ build_p_grass_200,family = binomial, data = data.frame(.))),
    log_build_p_grass_300= map(., ~glm(damage_binary ~ build_p_grass_300,family = binomial, data = data.frame(.))),
    
    log_build_p_soil_10 = map(., ~glm(damage_binary ~ build_p_soil_10, family = binomial, data = data.frame(.))),
    log_build_p_soil_100= map(., ~glm(damage_binary ~ build_p_soil_100,family = binomial, data = data.frame(.))),
    log_build_p_soil_200= map(., ~glm(damage_binary ~ build_p_soil_200,family = binomial, data = data.frame(.))),
    log_build_p_soil_300= map(., ~glm(damage_binary ~ build_p_soil_300,family = binomial, data = data.frame(.))),
    
    log_build_p_water_10 = map(., ~glm(damage_binary ~ build_p_water_10, family = binomial, data = data.frame(.))),
    log_build_p_water_100= map(., ~glm(damage_binary ~ build_p_water_100,family = binomial, data = data.frame(.))),
    log_build_p_water_200= map(., ~glm(damage_binary ~ build_p_water_200,family = binomial, data = data.frame(.))),
    log_build_p_water_300= map(., ~glm(damage_binary ~ build_p_water_300,family = binomial, data = data.frame(.))),
    
    log_build_p_buliding_10 = map(., ~glm(damage_binary ~ build_p_building_10, family = binomial, data = data.frame(.))),
    log_build_p_buliding_100= map(., ~glm(damage_binary ~ build_p_building_100,family = binomial, data = data.frame(.))),
    log_build_p_buliding_200= map(., ~glm(damage_binary ~ build_p_building_200,family = binomial, data = data.frame(.))),
    log_build_p_buliding_300= map(., ~glm(damage_binary ~ build_p_building_300,family = binomial, data = data.frame(.))),
    
    log_build_p_road_10 = map(., ~glm(damage_binary ~ build_p_road_10, family = binomial, data = data.frame(.))),
    log_build_p_road_100= map(., ~glm(damage_binary ~ build_p_road_100,family = binomial, data = data.frame(.))),
    log_build_p_road_200= map(., ~glm(damage_binary ~ build_p_road_200,family = binomial, data = data.frame(.))),
    log_build_p_road_300= map(., ~glm(damage_binary ~ build_p_road_300,family = binomial, data = data.frame(.))),
    
    log_build_p_otherpaved_10 = map(., ~glm(damage_binary ~ build_p_otherpaved_10, family = binomial, data = data.frame(.))),
    log_build_p_otherpaved_100= map(., ~glm(damage_binary ~ build_p_otherpaved_100,family = binomial, data = data.frame(.))),
    log_build_p_otherpaved_200= map(., ~glm(damage_binary ~ build_p_otherpaved_200,family = binomial, data = data.frame(.))),
    log_build_p_otherpaved_300= map(., ~glm(damage_binary ~ build_p_otherpaved_300,family = binomial, data = data.frame(.))),
    
    log_build_p_shrub_10 = map(., ~glm(damage_binary ~ build_p_shrub_10, family = binomial, data = data.frame(.))),
    log_build_p_shrub_100= map(., ~glm(damage_binary ~ build_p_shrub_100,family = binomial, data = data.frame(.))),
    log_build_p_shrub_200= map(., ~glm(damage_binary ~ build_p_shrub_200,family = binomial, data = data.frame(.))),
    log_build_p_shrub_300= map(., ~glm(damage_binary ~ build_p_shrub_300,family = binomial, data = data.frame(.)))

    ) %>%
  select(-data) %>% 
  rowid_to_column() %>% 
  pivot_longer(-rowid, names_to = 'model_name', values_to = 'model') %>% 
  select(-rowid) %>% 
  separate(col = model_name, into = c('trash_1', 'trash_2', 'trash_3', 'var', 'dist'), remove = FALSE, convert = TRUE) %>% 
  select(-starts_with('trash_')) %>% 
  mutate(#smry = map(model, summary),
         dsqr = map(model, Dsquared),
         AIC = map(model, AIC),
         r2 = map(model, performance::r2)); toc()


# extract goodness of fit measures
(
  gof <- uni_log_mods %>% 
    select(var, dist, AIC, dsqr, r2) %>% 
    unnest(dsqr, AIC, r2) %>% unnest(r2)
)

# which distance provides the best fit per variable?
(
  best_dist <- gof %>% 
    group_by(var) %>% 
    summarise(max_d = dist[which.max(dsqr)],
              max_r = dist[which.max(r2)],
              min_AIC=dist[which.min(AIC)]) 
)


```


### B Whats up with overhang measures?
```{r}
# build %>%
#   select(build_has_tree_overhang, build_overhang_ht, build_perc_overhang) %>%
#   tab_corr(., triangle = 'lower')

# corr.test(build$build_overhang_ht, build$build_perc_overhang)
# 
# build %>% 
#   ggplot(aes(as.factor(build_has_tree_overhang), build_overhang_ht)) +
#   geom_boxplot() +
#   theme_bw(16)
# 
# build %>% 
#   ggplot(aes(as.factor(build_has_tree_overhang), build_perc_overhang)) +
#   geom_boxplot() +
#   theme_bw(16)


tic(); overhang_uni_log_mods <- build %>%
  #nest(data = everything()) %>%
  nest() %>% 
  mutate(
    log_build_has_tree_overhang = map(., ~glm(damage_binary ~ as.factor(build_has_tree_overhang), family = binomial, data = data.frame(.))),
    log_build_overhang_ht       = map(., ~glm(damage_binary ~ build_overhang_ht,family = binomial, data = data.frame(.))),
    log_build_perc_overhang     = map(., ~glm(damage_binary ~ build_perc_overhang,family = binomial, data = data.frame(.)))
    ) %>%
  select(-data) %>% 
  rowid_to_column() %>% 
  pivot_longer(-rowid, names_to = 'model_name', values_to = 'model') %>% 
  select(-rowid) %>% 
  mutate(#smry = map(model, summary),
    lab = c('has_overhang', 'overhang_ht', 'perc_overhang'),
         dsqr = map(model, Dsquared),
         AIC = map(model, AIC),
         r2 = map(model, performance::r2)) %>%
  unnest(dsqr, AIC, r2) %>%
  unnest(r2) %>% 
  select(-model); toc()

overhang_uni_log_mods %>% View() 

# so none are very predictive and they are only partially related (ex build_has_tree_overhang, build_overhang_ht)
```




#### maximal logistic models
```{r}
build %>% map(., ~sum(is.na(.)))


build_bin_maximal <- build %>% #glimpse()
  # mutate(build_has_tree_overhang = as.factor(build_has_tree_overhang)) %>%
  tidylog::select(damage_binary,
         # landscape position
         build_Mean_elev_30m,
         build_Mean_slope_100m_DEM,
         build_Mean_aspect_100m_DEM,
         build_Mean_distroad, # TODO what are these? Are they correlated, pick between them?
         build_Mean_builddens,
         build_Mean_distall_road,
         
         # dist to _
         build_min_dist_tree,
         build_min_dist_shrub,
         build_dist_to_build_ft,
         
         # overhang
         #build_has_tree_overhang, # FIXME
         build_overhang_ht,
         build_perc_overhang,
         
         # building landcover buffers (should match `best_dist`)
         build_p_tree_100,
         build_p_grass_300,
         build_p_soil_200,
         build_p_water_100,
         build_p_building_300,
         build_p_road_300,
         build_p_otherpaved_300,
         build_p_shrub_300,
         
         # parcel land cover summaries
         parcel_Can_P : parcel_Imperv_P
         
         # note there is no parcel year built!
         
         ) %>% 
  drop_na()

build_bin_maximal %>% map(., ~sum(is.na(.)))

log_r_maximal <- glm(damage_binary ~ ., data = build_bin_maximal, family = binomial) 

log_r_step <- glm(damage_binary ~ ., data = build_bin_maximal, family = binomial) %>%
  MASS::stepAIC(trace = FALSE)


tab_model(log_r_maximal, log_r_step,
          show.aic = TRUE, show.r2 = TRUE)

modEvA::Dsquared(log_r_maximal)
modEvA::Dsquared(log_r_step)


performance::compare_performance(log_r_maximal, log_r_step)

plot_model(log_r_maximal, vline.color = 'black') + ylim(0.2, 1.2) + theme_bw(12)

```





### C fit Jeffrey Evan's worked example: randomForest
```{r}
# modeled after and adapted from this page
# from https://evansmurphy.wixsite.com/evansspatial/random-forest-sdm

# abbridged from J Evan's second chunk (chunk 1 accesses data)
# library(randomForest)
# library(rfUtilities)

b <- 1001                                     # Number of Bootstrap replicates
# snip
# chunks 3 and 4 import raster dadta and extract variables from raster to absence/present point locations
# snip

build_bin_maximal

build_bin_maximal %>% tabyl(damage_binary)

# remove build_has_tree_overhang?
build_bin_maximal %<>% mutate(damage_binary = as.factor(damage_binary)) %>% data.frame()# PAY ATTENTION HERE
build_bin_maximal %>% glimpse


(cl <- multi.collinear(build_bin_maximal[,2:ncol(build_bin_maximal)], p = 0.05))

# tell us what's got to go!
for(l in cl) {
  cl.test <- build_bin_maximal[,-which(names(build_bin_maximal)==l)]
  print(paste("Remove variable", l, sep=": "))
  multi.collinear(cl.test, p=0.05)
}


build_bin_maximal <- build_bin_maximal[,-which(names(build_bin_maximal) %in% cl )] %>%
  data.frame()

# Chunk 8 makes depenent variable categorical (done). BUT it also checks for frequency of presense

# "We observe that the sample balance of presence locations is 33% thus, meeting the 1/3 rule for sample balance."

build_bin_maximal %>% tabyl(damage_binary)
print('12% - is this a problem?!?!')

# model selection
# ( rf.model <- rf.modelSel(x=sdata@data[,3:ncol(sdata@data)], y=sdata@data[,"Present"], imp.scale="mir", ntree=b) )

ptm <- proc.time() # clock in
(rf.model <- rf.modelSel(x=build_bin_maximal[,2:ncol(build_bin_maximal)],
                         y=build_bin_maximal[,"damage_binary"],
                         imp.scale="mir", ntree=b))
(proc.time() - ptm) / 60 # clock out, about 3 minutes
beepr::beep()


sel.vars <- rf.model$selvars

# OR, pick a model?
#sel.vars <- rf.model$parameters[[4]] # DECISION POINT

# run a model
ptm <- proc.time() # clock in
( rf.fit <- randomForest(y=build_bin_maximal[,"damage_binary"], x=build_bin_maximal[,sel.vars],
                         ntree=b,
                         importance=TRUE, norm.votes=TRUE, proximity=TRUE) )
(proc.time() - ptm) / 60 # clock out, about 3 minutes
beepr::beep()


# run a model with ALL PREDICTORS
ptm <- proc.time() # clock in
( rf.fit_all <- randomForest(y=build_bin_maximal[,"damage_binary"],
                             x=build_bin_maximal[,2:ncol(build_bin_maximal)],
                         ntree=b,
                         importance=TRUE, norm.votes=TRUE, proximity=TRUE) )
(proc.time() - ptm) / 60 # clock out, about 4 minutes
beepr::beep()


ptm <- proc.time() # clock in
( imbal <- randomForestSRC::imbalanced(damage_binary~., data=build_bin_maximal) )
(proc.time() - ptm) / 60 # clock out, about 3 minutes
beepr::beep()

# skipping chunks 13 and 14: predicted raster map.. doesn't really apply here.

# model fit!
rf.pred <- predict(rf.fit, build_bin_maximal[,sel.vars], type="response")

rf.prob <- as.data.frame(predict(rf.fit, build_bin_maximal[,sel.vars], type="prob"))

obs.pred <- data.frame(cbind(Observed=as.numeric(as.character(build_bin_maximal[,"damage_binary"])),
                             PRED=as.numeric(as.character(rf.pred)), Prob1=rf.prob[,2],
                             Prob0=rf.prob[,1]) )

op <- (obs.pred$Observed == obs.pred$PRED)



( pcc <- (length(op[op == "TRUE"]) / length(op))*100 )



# library(verification)

roc.plot(obs.pred[,"Observed"], obs.pred[,"Prob1"])


# model validation
ptm <- proc.time() # clock in
( rf.perm <- rf.significance(rf.fit, build_bin_maximal[,sel.vars], nperm = 99, ntree = 1001) )
(proc.time() - ptm) / 60 # clock out, 59 minutes with 99 permutations
beepr::beep()

saveRDS(rf.perm, file = "saved_sessions/rf.perm.rds")

# cross validation
ptm <- proc.time() # clock in
( rf.cv <- rf.crossValidation(rf.fit, build_bin_maximal[,sel.vars], p=0.10, n=99, ntree=1001) )
(proc.time() - ptm) / 60 # clock out, about 143 minutes (2.38 hours) with 999 permutations
beepr::beep()

# so this model isn't very good

saveRDS(rf.cv, file = "saved_sessions/rf.perm.rds")

# so the number of trees is probably over kill at 1000? 75 sufficient?
plot(rf.fit, main="Bootstrap Error Convergence")


# variable importance
p <- as.matrix(rf.fit$importance[,3])   
ord <- rev(order(p[,1], decreasing=TRUE)[1:dim(p)[1]]) 


png(file = paste0(getwd(), '/figures/var_imp_', gsub('[[:punct:]]', '_', Sys.time()), '.png'))
dotchart(p[ord,1], main="Scaled Variable Importance", pch=19)  
dev.off()


p <- as.matrix(rf.fit_all$importance[,3])   
ord <- rev(order(p[,1], decreasing=TRUE)[1:dim(p)[1]]) 


png(file = paste0(getwd(), '/figures/var_imp_all_', gsub('[[:punct:]]', '_', Sys.time()), '.png'))
dotchart(p[ord,1], main="Scaled Variable Importance", pch=19)  
dev.off()

(all_vars <- names(build_bin_maximal))

# `%nin%` <- Negate(`%in%`) # custom function
# 
# # drop the binary
# (all_vars <- all_vars[all_vars %nin% 'build_has_tree_overhang'])

par(mfrow=c(1,1))
for(i in all_vars[2:length(all_vars)]) {
  print(i)
  png(file = paste0(getwd(), '/figures/', i, '_',
                  gsub('[[:punct:]]', '_', Sys.time()), '.png'))
  rf.partial.prob(rf.fit_all, build_bin_maximal[,all_vars], i, "1", smooth="spline", raw.line=FALSE)
  dev.off()
}  



# par(mfrow=c(2,2))
# for(i in sel.vars[1:4]) {
#   rf.partial.prob(rf.fit, build_rforest_cat[,sel.vars], i, "1", smooth="spline", raw.line=FALSE)
# }  


for(i in sel.vars) {
png(file = paste0(getwd(), '/figures/', i, '_',
                  gsub('[[:punct:]]', '_', Sys.time()), '.png'))
rf.partial.prob(rf.fit, build_bin_maximal[,sel.vars], i, "1", smooth="spline", raw.line=FALSE)
dev.off()
  }  
beepr::beep()
```







#### i random forest readings

Thanks Sebastian Martinuzzi

https://evansmurphy.wixsite.com/evansspatial/random-forest-sdm

https://cran.r-project.org/web/packages/rfUtilities/index.html
(Utilities for Random Forest model selection, class balance correction, significance test, cross validation and partial dependency plots.)

https://www.rdocumentation.org/packages/randomForestSRC/versions/2.9.3/topics/imbalanced.rfsrc
(Imbalanced Two Class Problems)

Old, but important
https://statistics.berkeley.edu/sites/default/files/tech-reports/666.pdf

A review paper on "A survey on addressing high-class imbalance in big data"
https://link.springer.com/article/10.1186/s40537-018-0151-6

https://towardsdatascience.com/random-forest-in-r-f66adf80ec9

http://www.sthda.com/english/articles/36-classification-methods-essentials/150-stepwise-logistic-regression-essentials-in-r/#:~:text=The%20stepwise%20logistic%20regression%20can,ref(stepwise%2Dregression))

https://compstat-lmu.github.io/iml_methods_limitations/pdp.html

https://www.blopig.com/blog/2017/04/a-very-basic-introduction-to-random-forests-using-r/




```{r, citations}
lapply(packages, citation)
```


Last knit on `r format(Sys.time())`


```{r}
system.time(save.image(file = paste0('saved_sessions/wui_r_models_', gsub('[[:punct:]]', '-', Sys.time()), '.RData')))
```

